<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on</title><link>https://zenuo.github.io/</link><description>Recent content in Home on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 15 May 2019 10:37:11 +0800</lastBuildDate><atom:link href="https://zenuo.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>（续）如何从堆转储中恢复导致OOM的Excel文件</title><link>https://zenuo.github.io/posts/2023/06/11/%E7%BB%AD%E5%A6%82%E4%BD%95%E4%BB%8E%E5%A0%86%E8%BD%AC%E5%82%A8%E4%B8%AD%E6%81%A2%E5%A4%8D%E5%AF%BC%E8%87%B4oom%E7%9A%84excel%E6%96%87%E4%BB%B6/</link><pubDate>Sun, 11 Jun 2023 00:00:00 +0800</pubDate><guid>https://zenuo.github.io/posts/2023/06/11/%E7%BB%AD%E5%A6%82%E4%BD%95%E4%BB%8E%E5%A0%86%E8%BD%AC%E5%82%A8%E4%B8%AD%E6%81%A2%E5%A4%8D%E5%AF%BC%E8%87%B4oom%E7%9A%84excel%E6%96%87%E4%BB%B6/</guid><description>OOM发生后，我们可以通过堆转储文件分析出原因，那么我们能从堆转储文件里挖到更多细节吗？这一篇我们继续聊聊Excel OOM的事儿。
1 理论基础 根据MAT文档Analyzing Threads描述，hprof文件含有线程的栈以及栈里的局部变量：
Some heap dump formats (e.g. HPROF dumps from recent Java 6 VMs and IBM system dumps) contain information about the call stacks of threads, and the Java local objects per stack frame.
也就是说若程序实现是把excel解压缩之后的内容都加载到内存中的话，那么是可能提取出来的。
只要能找到内容，根据上文说到的xlxs结构，我们可以把xml等文本写到文本，再根据响应的文件结构存放，使用zip压缩后更名为xlsx文件，就算是提取了。
2 阅读源码 走一下POI的初始化workbook代码，可以发现构造器org.apache.poi.xssf.usermodel.XSSFWorkbook#XSSFWorkbook(org.apache.poi.openxml4j.opc.OPCPackage)的唯一入参的类型OPCPackage唯一实现是org.apache.poi.openxml4j.opc.ZipPackage，也许会有解压缩后的文件内容。
3 查看栈的局部变量 打开MAT的线程视图：
打开堆转储文件，根据之前的分析OOM是由线程http-nio-8080-exec-5引发的，我们导航到这个线程；点击以下按钮，展开这个线程的全部栈帧：
然后导航到方法org.apache.poi.xssf.usermodel.XSSFWorkbook#XSSFWorkbook(org.apache.poi.openxml4j.opc.OPCPackage)：
关注ZipPackage类型的变量，点击它左侧的按钮展开，再递归地展开，最终能找到字段zipEntries，对应的列表既是xlsx文件内包含的所有文件：
4 提取内容到xlsx文件 找到了xml数据，我们如何构造一个xlsx文件呢，根据上文提到的xlsx文件结构：</description></item><item><title>一次由Excel解析导致的OOM及其排查</title><link>https://zenuo.github.io/posts/2023/05/28/%E4%B8%80%E6%AC%A1%E7%94%B1excel%E8%A7%A3%E6%9E%90%E5%AF%BC%E8%87%B4%E7%9A%84oom%E5%8F%8A%E5%85%B6%E6%8E%92%E6%9F%A5/</link><pubDate>Sun, 28 May 2023 00:00:00 +0000</pubDate><guid>https://zenuo.github.io/posts/2023/05/28/%E4%B8%80%E6%AC%A1%E7%94%B1excel%E8%A7%A3%E6%9E%90%E5%AF%BC%E8%87%B4%E7%9A%84oom%E5%8F%8A%E5%85%B6%E6%8E%92%E6%9F%A5/</guid><description>在业务系统功能中，不难见到上传Excel文件批量处理的场景，由于Excel文件的复杂性，存在很多容易出问题的可能性。
1 复现 我们通过一个简化的SpringBoot工程来演示有问题的代码、如何通过Eclipse Memory Analyzer工具分析堆转储文件，来定位到问题代码。
工程代码excel-oom.zip：
. ├── pom.xml ├── readme.md ├── src │ └── main │ └── java │ └── demo │ └── exceloom │ └── DemoExcelOomApplication.java ├── test.xlsx 在DemoExcelOomApplication.java中是我们关注的重点，使用Apache POI框架解析输入流，然后读取第一张表的每一行：
@RequestMapping(value = &amp;#34;import&amp;#34;, method = RequestMethod.POST) public ResponseEntity&amp;lt;String&amp;gt; importDataByPoi(MultipartFile file) throws Exception { Workbook workbook = WorkbookFactory.create(file.getInputStream()); // poi解析输入流 Sheet sheetOne = workbook.</description></item><item><title>基于redsocks2和pf的macOS透明代理</title><link>https://zenuo.github.io/fragments/2022-09-04-%E5%9F%BA%E4%BA%8Eredsocks2%E5%92%8Cpf%E7%9A%84macos%E9%80%8F%E6%98%8E%E4%BB%A3%E7%90%86/</link><pubDate>Sun, 04 Sep 2022 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2022-09-04-%E5%9F%BA%E4%BA%8Eredsocks2%E5%92%8Cpf%E7%9A%84macos%E9%80%8F%E6%98%8E%E4%BB%A3%E7%90%86/</guid><description>坑点：在[🧰EasyConnect in Dokcer](../2021-05-01-🧰EasyConnect in Dokcer/)中记录了在容器中运行Sangfor EasyConnect的步骤，并通过socks5代理来实现VPN使用，但这种方式具有一些局限性，仅对支持了socks5或http代理的程序有用。举一个不支持的例子：我在本地执行Java工程的单元测试时，单元测试中有访问zk的逻辑，尽管在JVM参数中添加了-DsocksProxySet=true -DsocksProxyHost=alpine -DsocksProxyPort=1080，但对访问zk底层的nio来说是无效的😢
还好在搜索到了这两篇文章，发现可以通过透明代理来支持这种场景：
macos使用redsocks做透明代理 macOS 透明代理配置 下面是我的操作步骤
1 编译、配置redsocks $ wget https://github.com/HaoH/redsocks/archive/release-0.68.tar.gz # 下载源码 $ tar -zxvf redsocks-release-0.68.tar.gz # 解压缩 $ cd redsocks-release-0.68 在我的电脑环境中，按照源码包的代码构建的结果，运行时会报错：
1662282301.374137 err redsocks.c:693 redsocks_connect_relay(...) [192.168.0.103:54303-&amp;gt;10.100.*.*:80]: red_connect_relay failed!!!: Protocol not available 1662282301.375292 err utils.c:154 red_prepare_relay(...) setsockopt: Protocol not available 可以看到报错在utils.c:154，经查阅资料，此种报错可以被忽略，所以修改代码：
$ vim utils.</description></item><item><title>使用Javac日志分析工程依赖</title><link>https://zenuo.github.io/fragments/2022-07-02-%E4%BD%BF%E7%94%A8javac%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E5%B7%A5%E7%A8%8B%E4%BE%9D%E8%B5%96/</link><pubDate>Sat, 02 Jul 2022 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2022-07-02-%E4%BD%BF%E7%94%A8javac%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E5%B7%A5%E7%A8%8B%E4%BE%9D%E8%B5%96/</guid><description>最近的工作是迁移一个历史遗留的工件（本文称之为工件A），首先需要梳理这个工件的消费者。初步计划按照下面步骤进行：
在代码搜索引擎（例如zoekt）里搜索： 关键词：工件A的java文件头部的import包名 文件类型：java 然后将所有结果的工程名进行排重，得到工件A的所有消费者工程。 Q:为什么不能直接搜索pom.xml文件中搜索工件A的名称呢？
A:因为存在工件A的消费者工件传递依赖工件A的情况，此种方法不能覆盖传递依赖的场景🥹
使用Intellij IDEA打开每个消费者工程，依次查看工件A中的服务接口（interface）的使用，菜单Navigatie&amp;gt;Declaration or Usages 记录工件A的服务接口存在被使用的方法（后续实现对应的替换接口） 🤨不过看第2、3个步骤看起来挺花时间的，还有其他办法吗？
目前想到的其他办法就是从编译器日志下手，查得可以使用-verbose选项，来实现：
Uses verbose output, which includes information about each class loaded and each source file compiled.
下面通过一个demo来演示：
使用maven创建工件A：
➜ mvn archetype:generate \ -DarchetypeGroupId=org.apache.maven.archetypes \ -DarchetypeArtifactId=maven-archetype-quickstart \ -DarchetypeVersion=RELEASE \ -DgroupId=demo.a \ -DartifactId=demo-a \ -Dversion=0.0.1-SNAPSHOT \ -DinteractiveMode=false 然后添加dto、service类：
➜ tree src/main/java/ src/main/java/ └── demo └── a ├── dto │ ├── ProductDto.</description></item><item><title>Spring StateMachine踩坑</title><link>https://zenuo.github.io/posts/2022/05/14/spring-statemachine%E8%B8%A9%E5%9D%91/</link><pubDate>Sat, 14 May 2022 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2022/05/14/spring-statemachine%E8%B8%A9%E5%9D%91/</guid><description>最近做的客户管理项目，其中客户的开户、合同变更、合同续签功能涉及到走审批（基于公司采购的致远OA系统），业务上有这些需求：
某个确定的功能（比如客户开户）由某些确定的状态、动作组成 某些状态流转取决于审批结果是通过还是拒绝 某个状态允许重新流转 为了实现这些需求，可以简单粗暴if-else硬编码来实现（画面太美），为了代码的可维护性，团队考虑引入某些开源框架来优化，先后调研了工作流框架Flowable、状态机框架Spring StateMachine，基于学习成本和运维成本考虑，决定引入更加轻量的Spring StateMachine。如何快速入手可以参考官网，先介绍一下我们的实践的宏观结构。</description></item><item><title>MySQL源码初探</title><link>https://zenuo.github.io/fragments/2022-04-25-mysql%E6%BA%90%E7%A0%81%E5%88%9D%E6%8E%A2/</link><pubDate>Mon, 25 Apr 2022 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2022-04-25-mysql%E6%BA%90%E7%A0%81%E5%88%9D%E6%8E%A2/</guid><description>首先需要本地编译，启用debug：
curl -o mysql-server-8.0.zip https://codeload.github.com/mysql/mysql-server/zip/refs/heads/8.0 mkdir /opt/source/mysql-server-8.0 unzip -d /opt/source/mysql-server-8.0 mysql-server-8.0.zip cd /opt/source/mysql-server-8.0/ mkdir bld cd bld cmake -DWITH_DEBUG=ON -DWITH_BOOST=/opt/source/boost_1_73_0 -DWITH_SSL=/usr/local/Cellar/openssl@1.1/1.1.1k/ -DADD_GDB_INDEX=ON -DCMAKE_BUILD_TYPE=Debug -DWITH_ARCHIVE_STORAGE_ENGINE=0 -DWITH_FEDERATED_STORAGE_ENGINE=0 -DSYSCONFDIR=/opt/app/mysql8/ .. make mysqld.exe!Per_thread_connection_handler::add_connection(Channel_info * channel_info) Line 394 at D:\opt\source\mysql-server\sql\conn_handler\connection_handler_per_thread.cc(394) mysqld.exe!Connection_handler_manager::process_new_connection(Channel_info * channel_info) Line 259 at D:\opt\source\mysql-server\sql\conn_handler\connection_handler_manager.cc(259) mysqld.exe!Connection_acceptor&amp;lt;Mysqld_socket_listener&amp;gt;::connection_event_loop() Line 66 at D:\opt\source\mysql-server\sql\conn_handler\connection_acceptor.h(66) mysqld.exe!socket_conn_event_handler(void * arg) Line 3228 at D:\opt\source\mysql-server\sql\mysqld.</description></item><item><title>理解Maven版本号</title><link>https://zenuo.github.io/posts/2022/01/30/%E7%90%86%E8%A7%A3maven%E7%89%88%E6%9C%AC%E5%8F%B7/</link><pubDate>Sun, 30 Jan 2022 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2022/01/30/%E7%90%86%E8%A7%A3maven%E7%89%88%E6%9C%AC%E5%8F%B7/</guid><description>来源Oracle® Fusion Middleware Developing Applications Using Continuous Integration
在使用Maven时，理解如何使用版本号是非常重要的。一个经过深思熟虑（well thought out）的策略可以大大简化你的依赖管理工作量。本文介绍了关于版本号在Maven中如何工作的重要概念。
包含以下三个部分：
Maven版本号如何工作 SNAPSHOT限定符 版本范围引用 1 Maven版本号如何工作 Maven的版本管理方案（scheme）使用以下的标准：
MajorVersion MinorVersion IncrementalVersion BuildNumber Qualifier 例如：
MajorVersion: 1.2.1 MinorVersion: 2.0 IncrementalVersion: 1.2-SNAPSHOT BuildNumber: 1.4.2-12 Qualifier: 1.2-beta-2 具有限定符的版本，旧于没有限定符的（发布版本），例如：1.2-beta-2旧于1.2。
带有不同限定符的相同版本，将会用基础的字符串比较，例如：1.2-beta-2新于1.2-alpha-6。
如果你在项目版本管理方案中没有遵循Maven的版本标准，那么对于版本比较，Maven将整个版本解释为一个简单的字符串。Maven及其核心插件将版本比较用于多项任务，最重要的是用于发布过程。
如果您使用非标准的版本管理方案，Maven发布和版本插件目标可能不会产生预期的结果。因为基本的字符串比较是在非标准的版本上进行的，所以在某些情况下，版本比较会错误地计算版本的顺序。
例如，Maven按以下顺序排列版本列表：
1.0.1.0 1.0.10.1 1.0.10.2 1.0.9.3 版本1.0.9.3应该在1.0.10.1和1.0.10.2之前，但意外的第四个字段（.3）迫使Maven将版本评估为一个字符串。
在Maven版本插件中可以看到这种对Maven影响的例子。Maven版本插件提供了以不同方式检查项目依赖关系的目标。其中一个有用的目标是versions:dependency-updates-report，它将检查项目的依赖层次，并报告哪些项目有较新的版本。当你在协调一个大型发布时，这个目标可以帮你找到依赖配置中的陈旧引用。如果Maven错误地识别了一个较新的版本，那么在插件中也会出现错误的报告。在前面的这个例子中，若你当前版本是1.0.10.2，那么这个插件将会将1.0.9.3报告成一个较新的版本。
如果你打算在你的依赖关系引用中使用版本范围，版本解析也是非常重要的。参见第3节，了解关于版本变化的信息。
2 SNAPSHOT限定符 Maven对待SNAPSHOT限定符的处理方式跟其他所有限定符不同。若一个版本号以-SNAPSHOT结尾，那么Maven会认为它是相关MajorVersion、MinorVersion或IncrementalVersion的 &amp;ldquo;尚未发布 &amp;ldquo;版本。</description></item><item><title>在macOS上使用VS Code阅读OpenJDK源码</title><link>https://zenuo.github.io/fragments/2021-09-25-%E5%9C%A8macos%E4%B8%8A%E4%BD%BF%E7%94%A8vs-code%E9%98%85%E8%AF%BBopenjdk%E6%BA%90%E7%A0%81/</link><pubDate>Sat, 25 Sep 2021 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2021-09-25-%E5%9C%A8macos%E4%B8%8A%E4%BD%BF%E7%94%A8vs-code%E9%98%85%E8%AF%BBopenjdk%E6%BA%90%E7%A0%81/</guid><description>1 本地环境 操作系统：macOS Big Sur 11.5.2 XCode: 13A5155e JDK: openjdk version &amp;ldquo;11.0.9&amp;rdquo; VS Code: 1.60.2 2 下载OpenJDK工程 创建一个不含用户名相关（建议）的路径，例如/opt/source，将OpenJDK 11的工程克隆至本地：
cd /opt/source 将openjdk/jdk11u克隆到本地，🐢需要良好的网络环境，所以使用本地的代理；若不需要代理请忽略第一行：
https_proxy=socks5://localhost:1080 \ git clone https://github.com/openjdk/jdk11u.git ☕️建议喝杯水等会儿，等克隆完进入下个步骤。
3 为构建做配置 对于文件数量大、结构复杂的工程，一般会引入构建工具，来实现日常的开发、发布流程的标准化，如GNU Make、Apache Maven等，OpenJDK 11亦然。
对于开发阶段，要求构建耗时短（不进行程度高的优化）、构建结果便于调试（debug）、运行速度可以不那么快；对于发布阶段，要求可执行文件运行速度快，构建耗时可以稍微长点儿。OpenJDK的构建支持通过配置（configure脚本）参数--with-debug-level=&amp;lt;level&amp;gt;来指定构建结果的调试级别，来实现不同阶段对构建结果的debug支持程度。
由于本文介绍的是搭建阅读代码环境，所以是开发阶段，执行：
cd /opt/source/jdk11u bash ./configure \ #指定本地的JDK，对版本有要求 \ --with-boot-jdk=/opt/app/jdk-11.0.9+11/Contents/Home \ #若代码中存在编译时警告，使编译继续进行，不当做异常 \ --disable-warnings-as-errors \ #指定调试等级 \ --with-debug-level=slowdebug \ #调试符号将在构建过程中生成，它们将被保存在生成的二进制文件中。 \ --with-native-debug-symbols=internal 若本地环境满足构建需求，脚本正常结束，否则会有报错信息。</description></item><item><title>📦利用GitHub Actions自动构建TeX</title><link>https://zenuo.github.io/fragments/2021-09-02-%E5%88%A9%E7%94%A8github-actions%E8%87%AA%E5%8A%A8%E6%9E%84%E5%BB%BAtex/</link><pubDate>Thu, 02 Sep 2021 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2021-09-02-%E5%88%A9%E7%94%A8github-actions%E8%87%AA%E5%8A%A8%E6%9E%84%E5%BB%BAtex/</guid><description>笔者维护的一些TeX文档（简历等）通常是在安装好TeX发行版的主机上进行编辑和构建，当无法访问这些主机的时候，该怎么操作呢？🤔
联想到GitHub的Actions功能，可以用来做CI/CD，也许有戏。通过搜索，找到了xu-cheng/latex-action，可以达到通过git push操作来触发构建，并将构建的pdf文件打包放置到Workflow的Artifacts中。
假设需要被编译的TeX文件的相对路径为resume.tex，编译器是XeLaTeX，那么可以用下面的workflow描述文件来达到目的：
name: Build LaTeX document on: [push] jobs: build_latex: runs-on: ubuntu-latest steps: - name: Set up Git repository uses: actions/checkout@v2 - name: Compile LaTeX document uses: xu-cheng/latex-action@v2 with: root_file: resume.tex pre_compile: &amp;#34;fc-list :lang=zh&amp;#34; latexmk_use_xelatex: true - uses: actions/upload-artifact@v2 with: name: PDF path: resume.pdf 放置到仓库的.github/workflows/路径，再触发push即可。</description></item><item><title>lsof之外的查询使用指定文件的进程的方法</title><link>https://zenuo.github.io/fragments/2021-08-30-lsof%E4%B9%8B%E5%A4%96%E7%9A%84%E6%9F%A5%E8%AF%A2%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AE%9A%E6%96%87%E4%BB%B6%E7%9A%84%E8%BF%9B%E7%A8%8B%E7%9A%84%E6%96%B9%E6%B3%95/</link><pubDate>Mon, 30 Aug 2021 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2021-08-30-lsof%E4%B9%8B%E5%A4%96%E7%9A%84%E6%9F%A5%E8%AF%A2%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AE%9A%E6%96%87%E4%BB%B6%E7%9A%84%E8%BF%9B%E7%A8%8B%E7%9A%84%E6%96%B9%E6%B3%95/</guid><description>在主机上查看使用某个文件的使用情况时，可能会直接想到使用lsof，但不是所有主机都安装了该工具，这时你可以使用下面的命令:
find /proc -regex &amp;#39;\/proc\/[0-9]+\/fd\/.*&amp;#39; -type l -lname &amp;#34;*关键词*&amp;#34; -printf &amp;#34;%p -&amp;gt; %l\n&amp;#34; 2&amp;gt; /dev/null 它通过搜索类Unix系统的process information pseudo-filesystem，来达到目的
参考 proc(5) — Linux manual page Scott&amp;rsquo;s answer</description></item><item><title>📷在树莓派上访问iPhone拍摄的照片与视频</title><link>https://zenuo.github.io/fragments/2021-07-11-%E5%9C%A8%E6%A0%91%E8%8E%93%E6%B4%BE%E4%B8%8A%E8%AE%BF%E9%97%AEiphone%E6%8B%8D%E6%91%84%E7%9A%84%E7%85%A7%E7%89%87%E4%B8%8E%E8%A7%86%E9%A2%91/</link><pubDate>Sun, 11 Jul 2021 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2021-07-11-%E5%9C%A8%E6%A0%91%E8%8E%93%E6%B4%BE%E4%B8%8A%E8%AE%BF%E9%97%AEiphone%E6%8B%8D%E6%91%84%E7%9A%84%E7%85%A7%E7%89%87%E4%B8%8E%E8%A7%86%E9%A2%91/</guid><description>我的iPhone照片与视频的体积超过了免费iCloud提供的5GB，但又不喜欢（qiong）订阅iCloud的付费扩容计划，所以看看是否能将这些照片与视频备份到树莓派挂载的硬盘上。
硬件情况：
树莓派硬件：树莓派2B 树莓派软件：Arch Linux ARM iPhone硬件：iPhone 11 iPhone软件：14.6 下面是我的操作步骤：
为了安装ifuse，需要安装yay，但我的树莓派的pacman源并未收纳yay，所以参考官方文档构建后安装：
sudo pacman -S --needed git base-devel # 科学上网，避免网络不畅通导致的时间浪费 export http_proxy=socks5://127.0.0.1:1080;export https_proxy=socks5://127.0.0.1:1080;export ALL_PROXY=socks5://127.0.0.1:1080 git clone https://aur.archlinux.org/yay.git cd yay makepkg -si 安装ifuse：
yay -S ifuse 选择ifuse，而不是ifuse-git，然后会请求安装一些依赖，允许即可。编译🧬安装需要一些时间，耐心等待～
安装完成后，运行usbmuxd服务：
sudo systemctl start usbmuxd.service 若运行成功，此时使用usb连接树莓派与iPhone，iPhone上会弹窗询问是否信任该计算机，选择信任即可（这还用说？😂）
确认iPhone连接成功：
idevicepair validate 创建一个挂载点：
mkdir ~/phone 挂载iPhone文件系统：
ifuse ~/phone 使用rsync同步到硬盘（挂载点为**/opt/d01/**）：</description></item><item><title>📦VirtualBox的端口转发功能</title><link>https://zenuo.github.io/fragments/2021-07-01-virtualbox%E7%9A%84%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91%E5%8A%9F%E8%83%BD/</link><pubDate>Thu, 01 Jul 2021 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2021-07-01-virtualbox%E7%9A%84%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91%E5%8A%9F%E8%83%BD/</guid><description>在使用VirtualBox时，若需要从主机访问虚拟机，我通常的做法是添加一个host only的网络适配器，然后通过这个适配器的IP访问。
但同样的需求也可通过配置NAT适配器上的端口转发规则来实现，更加轻量化。
例如，我需要访问虚拟机的22和53124端口，配置如下，即可通过访问127.0.0.1的53124端口来访问虚拟机的53124端口，通过访问127.0.0.1的53125端口来访问虚拟机的22端口。</description></item><item><title>🧰EasyConnect in Dokcer</title><link>https://zenuo.github.io/fragments/2021-05-01-easyconnect-in-dokcer/</link><pubDate>Sat, 01 May 2021 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2021-05-01-easyconnect-in-dokcer/</guid><description>Sangfor EasyConnect是一款专有的VPN解决方案，官方支持多种平台的客户端；但该软件目前存在以下的几种行为：
配置一个开机自动启动的守护进程EasyMonitor 安装CA根证书 为了避免上述两种情况对本地系统造成不良影响，尝试寻找方法将EasyConnect运行在受控的容器内。所幸在Hagb/docker-easyconnect找到了，该仓库介绍了一种在Docker内运行EasyConnect的方案。通过该方案，在此记录下我的实践过程。
1 运行容器 在Docker宿主机为Alpine Linux时，映射到Docker宿主机的端口，无法在宿主机的外部网络（例如宿主机所在的局域网）访问，但可以在宿主机本地访问；
创建文件用于保存登录凭证，以实现auto login：
$ touch ~/.easyconn 从Docker镜像hagb/docker-easyconnect:cli创建一个名称为easyconnect的容器，并且将容器的1080端口映射到Docker宿主机的1080端口（1080只是一个示例值，可以是其他的；敲黑板，后续会用到）：
$ docker run --name easyconnect --device /dev/net/tun --cap-add NET_ADMIN -ti -v $HOME/.easyconn:/root/.easyconn -e EC_VER=7.6.8 -e EXIT=1 -p 1080:1080 hagb/docker-easyconnect:cli 根据提示输入服务器URL、用户名、密码。
注意服务器URL末尾不需要反斜线（/），例如正确的https://vpn_host。
如果成功登入，则会提示：
user &amp;#34;xx&amp;#34; login successfully! 2 浏览器over proxy 浏览器运行时动态配置代理，可以通过SwitchyOmega来实现，须将Proxy设置为Docker宿主机的1080端口
浏览器启动时指定代理，若您是使用Chromium相关（比如Chrome），则可通过命令行启动：
$ chromium —proxy-server=socks5://${Docker宿主机IP}:1080 3 ssh over proxy 若您是使用Arch Linux，需要安装openbsd-netcat，而不是gnu-netcat</description></item><item><title>🔦如何使用GDB调试Nginx</title><link>https://zenuo.github.io/fragments/2021-02-28-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8gdb%E8%B0%83%E8%AF%95nginx/</link><pubDate>Sun, 28 Feb 2021 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2021-02-28-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8gdb%E8%B0%83%E8%AF%95nginx/</guid><description>本文主要描述在CentOS 7下，使用gdb调试Nginx
1 准备 下载nginx及其依赖项目源码，并解压：
curl -OL https://mirrors.sohu.com/nginx/nginx-1.19.7.tar.gz tar zxf nginx-1.19.7.tar.gz curl -OL https://ftp.pcre.org/pub/pcre/pcre-8.44.tar.gz tar zxf pcre-8.44.tar.gz curl -OL http://zlib.net/zlib-1.2.11.tar.gz tar zxf zlib-1.2.11.tar.gz 安装构建工具：
yum install gcc make 构建nginx:
cd /opt/app/nginx-1.19.7 ./configure --with-debug --with-cc-opt=&amp;#39;-O0 -g&amp;#39; --with-pcre=/opt/app/pcre-8.44 --with-zlib=/opt/app/zlib-1.2.11 --prefix=/opt/app/nginx make &amp;amp;&amp;amp; make install 修改nginx配置文件/opt/app/nginx/conf/nginx.conf:
# 最好是1，便于断点调试 worker_processes 1; events { worker_connections 1024; } http { include mime.</description></item><item><title>🧰Xcode中创建Swift Package并在Target中使用</title><link>https://zenuo.github.io/fragments/2020-10-31-xcode%E4%B8%AD%E5%88%9B%E5%BB%BAswift-package%E5%B9%B6%E5%9C%A8target%E4%B8%AD%E4%BD%BF%E7%94%A8/</link><pubDate>Sat, 31 Oct 2020 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2020-10-31-xcode%E4%B8%AD%E5%88%9B%E5%BB%BAswift-package%E5%B9%B6%E5%9C%A8target%E4%B8%AD%E4%BD%BF%E7%94%A8/</guid><description>首先在Xcode上创建一个Swift Package，菜单是File / New / Swift Package：
选择保存路径：
注意选择Add to：
选择Project / TARGETS，然后选择需要使用我们刚才创建的Swift Package的Target，然后选择General栏，在Framworks, Libraries, and Embedded Content中添加Package，如图： 选择Build Phases，在Dependencies添加Package，如图： 没有其他问题的话，您的Target能成功构建了。
Reference Organizing Your Code with Local Packages Adding Package Dependencies to Your App</description></item><item><title>🏎协变、逆变与不变</title><link>https://zenuo.github.io/posts/2020/10/19/%E5%8D%8F%E5%8F%98%E9%80%86%E5%8F%98%E4%B8%8E%E4%B8%8D%E5%8F%98/</link><pubDate>Mon, 19 Oct 2020 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2020/10/19/%E5%8D%8F%E5%8F%98%E9%80%86%E5%8F%98%E4%B8%8E%E4%B8%8D%E5%8F%98/</guid><description>Formal definition 摘自Covariance_and_contravariance_(computer_science)：
Within the type system of a programming language, a typing rule or a type constructor is:
covariant if it preserves the rdering of types (≤), which orders types from more specific to more generic; contravariant if it reverses this ordering; bivariant if both of these apply (i.e., both I&amp;lt;A&amp;gt; ≤ I&amp;lt;B&amp;gt; and I&amp;lt;B&amp;gt; ≤ I&amp;lt;A&amp;gt; at the same time);[1] variant if covariant, contravariant or bivariant; invariant or nonvariant if not variant.</description></item><item><title>📝基于xeCJK的TeX中文书写</title><link>https://zenuo.github.io/fragments/2020-07-19-%E5%9F%BA%E4%BA%8Execjk%E7%9A%84tex%E4%B8%AD%E6%96%87%E4%B9%A6%E5%86%99/</link><pubDate>Sun, 19 Jul 2020 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2020-07-19-%E5%9F%BA%E4%BA%8Execjk%E7%9A%84tex%E4%B8%AD%E6%96%87%E4%B9%A6%E5%86%99/</guid><description>1 安装环境 首先，您需要在本地安装一个TeX发行版，您可以选择大而全的MacTeX，为了节省本地空间，此处选择更小的发行版BasicTeX；若网速过慢，你可以选择国内院校提供的镜像，比如tuna（清华大学开源软件镜像站）；下载完成之后，运行程序，一路continue即可：
其次，使用tlmgr(the native TeX Live Manager)安装ctex和xecjk：
$ sudo tlmgr install ctex xecjk 2 Hello World 创建一个纯文本文件hello_world.tex，内容为：
\documentclass{article} \usepackage{xeCJK} \setCJKmainfont{STSong} \begin{document} Hello World!\\ 天地玄黃宇宙洪荒日月盈仄 \end{document} 保存文件，使用xelatex命令，输出pdf文件：
$ xelatex hello_world.tex 如果不出问题的话，输出的hello_world.pdf文件将会是：
3 参考 ** Smaller Download ** 全面总结如何在 LaTeX 中使用中文 (2020 最新版) - jdhao&amp;rsquo;s blog Include Chinese characters into article in Xelatex</description></item><item><title>📜读取MySQL的binlog</title><link>https://zenuo.github.io/fragments/2020-07-12-%E8%AF%BB%E5%8F%96mysql%E7%9A%84binlog/</link><pubDate>Sun, 12 Jul 2020 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2020-07-12-%E8%AF%BB%E5%8F%96mysql%E7%9A%84binlog/</guid><description>配置~/.my.cnf，开启binlog:
[mysqld] server-id=master-01 log-bin=mysql-bin 假设需要读取的binlog的文件是mysql-bin.000001，那么用下面命令即可根据开始结束时间输出可读日志：
mysqlbinlog --base64-output=DECODE-ROWS \ -v \ --start-datetime=&amp;#39;2020-07-12 21:33:10&amp;#39; \ --stop-datetime=&amp;#39;2020-07-12 21:40:10&amp;#39; \ mysql-bin.000001 输出如下：
/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/; /*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/; DELIMITER /*!*/; # at 4 #200712 21:24:18 server id 0 end_log_pos 123 CRC32 0x0d383a89 Start: binlog v 4, server v 5.7.29-log created 200712 21:24:18 at startup ROLLBACK/*!</description></item><item><title>🚇基于v2ray的科学上网</title><link>https://zenuo.github.io/fragments/2020-07-11-%E5%9F%BA%E4%BA%8Ev2ray%E7%9A%84%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/</link><pubDate>Sat, 11 Jul 2020 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2020-07-11-%E5%9F%BA%E4%BA%8Ev2ray%E7%9A%84%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/</guid><description>本篇笔记尝试从v2ray是什么、浅谈工作原理、运行服务端、运行客户端四个方面，通过少许命令和配置内容，让我们能迅速上手使用v2ray。
v2ray是什么？ Project V 是一个工具集合，它可以帮助你打造专属的基础通信网络。Project V 的核心工具称为V2Ray，其主要负责网络协议和功能的实现，与其它 Project V 通信。V2Ray 可以单独运行，也可以和其它工具配合，以提供简便的操作流程。
当然，v2ray是开源的，主要特性如下：
多入口多出口: 一个 V2Ray 进程可并发支持多个入站和出站协议，每个协议可独立工作。 可定制化路由: 入站流量可按配置由不同的出口发出。轻松实现按区域或按域名分流，以达到最优的网络性能。 多协议支持: V2Ray 可同时开启多个协议支持，包括 Socks、HTTP、Shadowsocks、VMess 等。每个协议可单独设置传输载体，比如 TCP、mKCP、WebSocket 等。 隐蔽性: V2Ray 的节点可以伪装成正常的网站（HTTPS），将其流量与正常的网页流量混淆，以避开第三方干扰。 反向代理: 通用的反向代理支持，可实现内网穿透功能。 多平台支持: 原生支持所有常见平台，如 Windows、Mac OS、Linux，并已有第三方支持移动平台。 浅谈工作原理 使用方式上，v2ray与其他代理shadowsocks工具类似，需要运行服务端和客户端两个实例(instance)。
常见的场景里，客户端运行在PC、手机等终端设备，服务端运行在云主机等服务器上。例如，我们若要在手机访问Google搜索引擎时，如果手机所在的网络禁用了Google的IP和域名，那么手机是无法直接访问的，需要由一个「手机可以访问的中间网络节点，且该节点能访问Google搜索引擎」的代理来起到一个桥梁的作用：手机的浏览器把HTTP请求发送到代理，代理把HTTP请求发送到Google搜索引擎，Google搜索引擎的HTTP响应会顺着这条路线最终返回给终端设备的浏览器。
在这里，v2ray和shadowsocks等网络工具就扮演了代理这个角色，但仅仅做网络流量代理是不够的，因为网络流量由网络协议承载，若协议的特征能被防火墙识别，被识别就意味着防火墙有理由把我们的代理IP添加到黑名单中，导致无法终端设备无法访问这个代理；所以网络工具还需要支持不易被识别的网络协议，这些协议往往不被浏览器等应用支持，需要由网络工具自己来支持，所以用作代理的网络工具需要运行客户端和服务端。
浏览器等终端应用通过HTTP、Socks5等常见的网络协议把流量发送给代理客户端，客户端再通过特定的协议将流量发送给代理服务端；
在v2ray的设计中，终端应用到代理客户端的接口被称为Inbound（入口），代理服务端到实际目的地（例如Google搜索引擎）的接口被称为Outbound（出口）。
运行服务端 摘自Project V官网，并假设你的服务端运行在Linux操作系统上。
1 下载 V2Ray 预编译的压缩包可以在如下几个站点找到：
Github Release: github.com/v2ray/v2ray-core Github 分流: github.</description></item><item><title>🧯记一次环境变量导致的中文乱码</title><link>https://zenuo.github.io/posts/2020/06/14/%E8%AE%B0%E4%B8%80%E6%AC%A1%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%AF%BC%E8%87%B4%E7%9A%84%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/</link><pubDate>Sun, 14 Jun 2020 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2020/06/14/%E8%AE%B0%E4%B8%80%E6%AC%A1%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%AF%BC%E8%87%B4%E7%9A%84%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/</guid><description>一、现象 使用本地ssh访问服务器 重启应用 查询主数据商品信息的内容中文乱码 jinfo命令输出file.encoding = ANSI_X3.4-1968 使用Xshell访问服务器 重启应用 jinfo命令输出file.encoding = UTF-8 查询主数据商品信息的内容中文恢复正常 二、为什么乱码 猜想 初步猜想是本地ssh和Xshell的ssh会话的locale会话不同，导致重启后的JVM默认字符集的不同导致了乱码，使用下面的代码片段来验证：
import java.nio.charset.Charset; public class Main { public static void main(String[] args) { System.out.println(System.getProperty(&amp;#34;file.encoding&amp;#34;)); System.out.println(Charset.defaultCharset()); } } 思路：分别使用本地ssh和Xshell访问服务器，查看locale并执行代码片段
使用本地ssh访问：
[服务器 ~]$ locale locale: Cannot set LC_CTYPE to default locale: No such file or directory # 异常信息 locale: Cannot set LC_ALL to default locale: No such file or directory # 异常信息 LANG=en_US.</description></item><item><title>💼命令Command设计模式在设计API时的运用</title><link>https://zenuo.github.io/posts/2020/04/05/%E5%91%BD%E4%BB%A4command%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%9C%A8%E8%AE%BE%E8%AE%A1api%E6%97%B6%E7%9A%84%E8%BF%90%E7%94%A8/</link><pubDate>Sun, 05 Apr 2020 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2020/04/05/%E5%91%BD%E4%BB%A4command%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%9C%A8%E8%AE%BE%E8%AE%A1api%E6%97%B6%E7%9A%84%E8%BF%90%E7%94%A8/</guid><description>你是否使用过一些类似于这样的API？
命令模式是一种行为设计模式，它可将请求转换为一个包含于请求相关的所有信息的独立对象。改装换让你根据不同的请求将方法参数化，延迟请求执行或将其放入队列中，且能实现可撤销操作。- https://refactoringguru.cn/design-patterns/command</description></item><item><title>🧙🏽‍♂️新人和非技术人员也能看懂的网站应用拓展</title><link>https://zenuo.github.io/posts/2020/01/19/%EF%B8%8F%E6%96%B0%E4%BA%BA%E5%92%8C%E9%9D%9E%E6%8A%80%E6%9C%AF%E4%BA%BA%E5%91%98%E4%B9%9F%E8%83%BD%E7%9C%8B%E6%87%82%E7%9A%84%E7%BD%91%E7%AB%99%E5%BA%94%E7%94%A8%E6%8B%93%E5%B1%95/</link><pubDate>Sun, 19 Jan 2020 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2020/01/19/%EF%B8%8F%E6%96%B0%E4%BA%BA%E5%92%8C%E9%9D%9E%E6%8A%80%E6%9C%AF%E4%BA%BA%E5%91%98%E4%B9%9F%E8%83%BD%E7%9C%8B%E6%87%82%E7%9A%84%E7%BD%91%E7%AB%99%E5%BA%94%E7%94%A8%E6%8B%93%E5%B1%95/</guid><description>翻译自Scaling webapps for newbs &amp;amp; non-techies ，作者Wolfram Hempel
拓展是什么？ 拓展分为横向与纵向；简单来说：
当然，你也可以称之为水平拓展与垂直拓展。
横向拓展（horizontal scaling）：并行地运行很多进程； 纵向拓展（vertical scaling）：在更强大的计算机上运行同样的程序； 今天，很少有人再纵向拓展，因为：
计算机的价格随着性能的提高呈指数增长 一台计算机只能如此之快，这对一台计算机可以垂直扩展的范围施加了严格的限制 多核CPU意味着即使一台计算机都可以有效地并行化-那么为什么不从一开始就并行化呢？ 单个服务器+数据库 这也许是你的后端最开始的样子。一个单独的应用服务器运行着你的业务逻辑，一个数据库保存着数据。事情既简单又好，但是满足更高需求的此设置的唯一方法是在功能更强大的计算机上运行它-不好。
添加一个反向代理 添加一个反向代理是你准备架构的第一步。想一下酒店前台，也许你可以直接将客人带到房间——但是，你真的需要一位中间人来校验一位客人是否被允许进入，是否她的所有文件都齐全并且准备前去一个存在的房间。而且如果房间关闭，您希望有人用友好的声音告诉客人，而不是让他们陷入困境。这就是一个反向代理所做的工作。通常，代理只是一个接收和转发请求的过程。通常，这些请求将从我们的服务器发送到Internet。但是这里的请求来自互联网，需要路由到我们的服务器，因此我们将其称为“反向代理”。
这样的代理可以做这些事：
运行状况检查可确保我们的实际服务器仍在运行 路由将请求转发到正确的端点 身份验证（Authentication）可确保实际上已授予用户访问服务器的权限 防火墙确保用户只能访问他们被允许使用的网络部分 以及更多。
引入一个负载均衡器 反向代理通常可以有负载均衡的功能。负载均衡的概念很简单，想象一下现在有100位用户需要支付，但是你目前只有一台支付服务器，一台支付服务器最多只能支持50位用户支付，你可以同时运行两个支付服务器。
一个负载均衡器的工作就是将请求按照配置的策略分到两个服务器，比如平均分配、按权重分配等等。
增加数据服务器 使用负载均衡器可以将负载分担给多个应用服务器，你看出了目前的问题吗？你可以利用负载均衡将负载分担给成千上万的应用服务器，但是数据库服务器只有一个。那么我们可以用同样的方式来拓展数据库吗？不幸的是不可以。这里的问题是一致性（consistency），我们系统的所有部分都需要就使用的数据达成一致。不一致的数据将导致很多问题，例如订单被处理多次，从同一个账户中重复扣款，以此类推；那么我们如何在确保一致性的同时拓展数据库？
我们可以做的第一件事是将其分为多个部分。一部分专门负责接收和存储数据，其他所有部分负责检索存储的数据。这种解决方案也称为主/从设置与读写分离。此处的假设是读的操作比写的操作次数多。此解决方案的优点是，可以保证一致性，因为数据仅写入单个实例，并从一个实例（从写入到读取）从那里流动。缺点是我们仍然只有一个数据库实例要写入。这对于中小型Web项目是可以的，但是如果您运行Facebook，它就不会这样做。
微服务 到目前为止，我们只处理一台服务器，该服务器可以完成所有工作：处理付款，订单，库存，为网站提供服务，管理用户帐户等等。这不一定是一件坏事-单个服务器意味着较低的复杂性，因此对我们的开发人员而言，头痛更少。但是随着规模的扩大，事情开始变得复杂和低效：
我们服务器的不同部分被不同程度地利用-对于每个用户登录，可能要处理几百次浏览量和要提供的资产，但是所有操作都是由同一台服务器完成的 我们的开发团队会随着我们的应用程序的发展而增长-但是，随着越来越多的开发人员在同一台服务器上工作，他们更有可能踩到彼此的脚趾。 仅拥有一台服务器意味着每当我们要上线新版本时，都必须完成所有工作并进行处理。每当一个团队迅速想要发布更新时，这就会导致危险的相互依赖，而另一团队仅完成了一半的工作。 应对这些挑战的解决方案是一种架构范例，已席卷开发人员：微服务。这个想法很简单-将您的服务器分解为功能单元，并将它们部署为单独的，相互连接的微型服务器。这有很多好处：
每个服务可以单独拓展，使我们能够更好地适应需求 开发团队可以独立工作，每个团队都负责各自微服务的生命周期（创建，部署，更新等） 每个微服务都可以使用自己的资源，例如自己的数据库 缓存和CDN 有什么比提高工作效率更好？根本不用工作！我们的网络应用程序很大一部分是由静态资产组成的，这些资产是永远不变的，例如图像，JavaScript和CSS文件，某些产品的预渲染目标网页等。无需在每个请求上重新计算或保留这些资产，我们可以使用“缓存”-一种小型存储，仅记住最后的结果并将其分发给所有感兴趣的人，而不会打扰底层服务器。
“内容交付网络”（CDN）-遍布世界各地的大量缓存，被称为缓存的“升级版”。这样一来，我们就可以从用户附近的商店向用户提供内容，而不必每次都在全球范围内发送数据。
消息队列 您去过游乐园吗？您只是走到售票柜台买票吗？可能不是-您最终可能会排队等待。政府机构，邮局和游乐园入口都是“次容量并行性”概念的绝佳示例-是的，它们是并行的：多个售票亭可以同时出售门票-但似乎永远不足以立即为每个人提供服务，结果，队列开始形成。</description></item><item><title>🧷关于技术写作</title><link>https://zenuo.github.io/posts/2020/01/17/%E5%85%B3%E4%BA%8E%E6%8A%80%E6%9C%AF%E5%86%99%E4%BD%9C/</link><pubDate>Fri, 17 Jan 2020 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2020/01/17/%E5%85%B3%E4%BA%8E%E6%8A%80%E6%9C%AF%E5%86%99%E4%BD%9C/</guid><description>翻译自Notes on Technical Writing，作者Marcus Kazmierczak。
在过去的一年中，我处理过WordPress的文档。在冻结发布期间，我开始做出贡献，以帮助开发人员过渡到新平台。我发现编写文档是我的最爱，并且乐于帮助和教育人们。尽管这不是我工作的主要部分，但我仍然在这里和那里找时间继续努力。
这一次，我阅读了有关技术写作和文档的各种资源。这些是我的笔记，既可以帮助我以后记住，又可以作为帮助我考虑现在写的工具。
Principles 在编写文档时，我尝试牢记以下原则：
技术写作的目的是帮助用户尽可能快和高效地完成任务。# People learn by doing, prefer to be shown and not told Get users to their first success quickly 有不止一种类型的文档 保持简单，用平易的语言 Tips Know your audience, know your purpose Put the most important information first Use bullet lists（使用无序符号列表） One idea per paragraph Edit, Edit, Edit（不断改进） Avoid Meta Writing 避免写关于文字的文章，文字没有自己的想法。 活跃的部分是作家和读者。</description></item><item><title>🧶Java线程与其对应的Linux进程号</title><link>https://zenuo.github.io/fragments/2019-09-23-java%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%85%B6%E5%AF%B9%E5%BA%94%E7%9A%84linux%E8%BF%9B%E7%A8%8B%E5%8F%B7/</link><pubDate>Mon, 23 Sep 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2019-09-23-java%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%85%B6%E5%AF%B9%E5%BA%94%E7%9A%84linux%E8%BF%9B%E7%A8%8B%E5%8F%B7/</guid><description>1 ps -eLf 首先，在Linux下，我们可以用ps -eLf命令看到java的线程号：
$ ps -eLf | grep java | grep -v grep opt 10801 1 10801 0 23 Sep19 ? 00:00:00 java -XX:+UseG1GC -XX:+UseStringDeduplication -Xms32m -Xmx32m -server -jar gogo.jar opt 10801 1 10806 0 23 Sep19 ? 00:00:02 java -XX:+UseG1GC -XX:+UseStringDeduplication -Xms32m -Xmx32m -server -jar gogo.jar opt 10801 1 10807 0 23 Sep19 ?</description></item><item><title>Java内存模型语义</title><link>https://zenuo.github.io/posts/2019/09/01/java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E8%AF%AD%E4%B9%89/</link><pubDate>Sun, 01 Sep 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2019/09/01/java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E8%AF%AD%E4%B9%89/</guid><description>翻译自Java Memory Model Pragmatics (transcript)
前言 Java内存模型（Java Memory Model，以后简称JMM）是Java规范中最复杂的部分，至少必须由程序库和运行时开发人员理解。不幸的是，它的措辞是这样的晦涩，以至于它需要一些资深人士为彼此破译它。当然，大多数开发人员并没有按照规定使用JMM规则，而是根据规则制定一些结构，或者更糟糕的是，盲目地复制高级开发人员的结构而不了解其适用性的限制。如果你是一个没有进入核心并发的普通人，你可以通过这篇文章，阅读高级书籍，如“Java Concurrency in Practice”。如果您是对所有这些工作感兴趣的高级人员之一，请继续阅读！
这篇文章是我今年在不同会议上发表的“Java Memory Model Pragmatics”演讲的记录副本，主要是俄语。世界上似乎提供可以容纳这么长时间的讨论的会议数量有限，并且需要在今年的JVMLS上为我的JMM研讨会揭露一些背景阅读，我决定将其转录。
我们将重用很多幻灯片，并且我将尝试基于它们构建叙述。当幻灯片不言自明时，有时候我会跳过没有叙述的。幻灯片有俄语和英语版本。下面的幻灯片是光栅化(rasterized)的，但具有很好的原始分辨率。
我要感谢Brian Goetz，Doug Lea，David Holmes，Sergey Kuksenko，Dmitry Chyuko，Mark Cooper，C.Scott Andreas，Joe Kearney以及其他许多人提供有用的评论和更正。关于最终字段的示例部分包含由Vladimir Sitnikov和Valentin Kovalenko解决的信息，并且是他们关于Final Fields Semantics的更大话题的摘录。
介绍 如果您阅读任何语言规范，您会发现它可以在逻辑上划分为两个相关但不同的部分。首先，一个非常简单的部分是语法，它描述了如何用该语言编写程序。其次，最大的部分是语义，它准确描述了特定语法结构的含义。语言规范通常通过执行程序的抽象机器的行为来描述语义，因此这种语言规范只是一个抽象的机器规范。
当您的语言具有存储（以变量，堆内存等形式）时，抽象机器也具有存储，您必须定义一组有关存储行为的规则。这就是我们所说的内存模型。如果您的语言没有显式存储（例如，您在调用上下文中传递数据），那么您的内存模型非常简单。在存储精通语言中，内存模型似乎回答了一个简单的问题：“特定读取可以观察到什么值？”
在顺序程序中，这似乎是一个空洞的问题：因为你有顺序程序，所以存储到内存中的是按照给定的顺序进行的，很明显读取应该按顺序观察最新的写入。这就是人们通常只为多线程程序遇到内存模型的原因，这个问题变得复杂了。然而，即使在连续的情况下，记忆模型也很重要（尽管它们通常在评估顺序的概念中巧妙地伪装）。
例如，C程序中未定义行为的臭名昭着的例子，它在序列点之间包含一些增量。该程序可以满足给定的断言，但也可以使其失败，或以其他方式召唤鼻子恶魔。有人可能会争辩说，这个程序的结果可能会有所不同，因为增量的评估顺序是不同的，但它不会解释，例如， 12的结果，当两个增量都没有看到另一个的写入值时。这是内存模型的关注点：每个增量应该看到什么值（并且通过扩展，它将存储什么）。
无论哪种方式，当提出实现特定语言的挑战时，我们可以采用两种方式之一：解释或将抽象机器编译到目标硬件。无论如何，解释和编译都通过Futamura Projections连接。 实际应用是解释器和编译器都负责模拟抽象机器。编译器通常被指责搞砸内存模型和多线程程序，但解释器也不能免疫。未能将解释器运行到抽象机器规范可能会导致内存模型违规。最简单的例子：将字段值缓存在解释器中的volatile读取上，您就完成了。这让我们进行了一次有趣的权衡。
编程语言仍然需要智能开发人员的原因是没有超级计算机编译器。 “Hyper”并不夸张：编译器工程中的一些问题是不可判定的，即使在理论上也是不可解决的，更不用说在实践中了。其他有趣的问题在理论上可行，但不实用。因此，为了使实用（优化）编译器成为可能，我们需要在语言中造成一些不便。硬件也是如此，因为（至少对图灵机而言）它只是二氧化硅中的算法。
为了详细说明这个想法，其余的讨论结构如下。
第一部分 Access Atomicity 我们想要什么 在JMM中最容易理解的是访问原子性保证。为了或多或少地严格指定，我们需要引入一些符号。在此幻灯片的示例中，您可以看到包含两列的表。该表示法如下。标题中的所有内容都已经发生：所有变量都已定义，所有初始化的商店都已提交，等等。列是不同的线程。在此示例中，线程1将一些值V2存储到全局变量t中。线程2读取变量，并断言读取值。在这里，我们要确保读取线程仅观察已知值，而不是之间的某些值。
我们有什么 对于理智的编程语言来说，这似乎是一个非常明显的要求：你怎么可能违反这个，为什么？这就是原因。 为了在并发访问下保持原子性，你必须至少让机器指令以给定宽度的操作数操作，否则原子性在指令级别被破坏：如果你需要将访问分成几个子访问，它们可以交错。但即使你有所需的宽度指令，它们仍然可以是非原子的：例如，对于PowerPC来说，2字节和4字节读取的原子性保证是未知的（它们暗示是原子的）。</description></item><item><title>🔍如何比较两个Collection含有相同的元素——来自Apache Commons Collections的实现</title><link>https://zenuo.github.io/fragments/2019-07-20-%E5%A6%82%E4%BD%95%E6%AF%94%E8%BE%83%E4%B8%A4%E4%B8%AAcollection%E5%90%AB%E6%9C%89%E7%9B%B8%E5%90%8C%E7%9A%84%E5%85%83%E7%B4%A0%E6%9D%A5%E8%87%AAapache-commons-collections%E7%9A%84%E5%AE%9E%E7%8E%B0/</link><pubDate>Sat, 20 Jul 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2019-07-20-%E5%A6%82%E4%BD%95%E6%AF%94%E8%BE%83%E4%B8%A4%E4%B8%AAcollection%E5%90%AB%E6%9C%89%E7%9B%B8%E5%90%8C%E7%9A%84%E5%85%83%E7%B4%A0%E6%9D%A5%E8%87%AAapache-commons-collections%E7%9A%84%E5%AE%9E%E7%8E%B0/</guid><description>您可以在org.apache.commons.collections4.CollectionUtils查看源代码。
Apache Commons Collections中的CollectionUtils#isEqualCollection(java.util.Collection&amp;lt;?&amp;gt;, java.util.Collection&amp;lt;?&amp;gt;)方法（及其重载），提供了比较两个Collection含有相同的元素的实现，其源代码如下：
/** * Returns {@code true} iff the given {@link Collection}s contain * exactly the same elements with exactly the same cardinalities. * &amp;lt;p&amp;gt; * That is, iff the cardinality of &amp;lt;i&amp;gt;e&amp;lt;/i&amp;gt; in &amp;lt;i&amp;gt;a&amp;lt;/i&amp;gt; is * equal to the cardinality of &amp;lt;i&amp;gt;e&amp;lt;/i&amp;gt; in &amp;lt;i&amp;gt;b&amp;lt;/i&amp;gt;, * for each element &amp;lt;i&amp;gt;e&amp;lt;/i&amp;gt; in &amp;lt;i&amp;gt;a&amp;lt;/i&amp;gt; or &amp;lt;i&amp;gt;b&amp;lt;/i&amp;gt;.</description></item><item><title>🗜原始类型与包装类型在自增运算上的区别</title><link>https://zenuo.github.io/fragments/2019-07-02-%E5%8E%9F%E5%A7%8B%E7%B1%BB%E5%9E%8B%E4%B8%8E%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B%E5%9C%A8%E8%87%AA%E5%A2%9E%E8%BF%90%E7%AE%97%E4%B8%8A%E7%9A%84%E5%8C%BA%E5%88%AB/</link><pubDate>Tue, 02 Jul 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2019-07-02-%E5%8E%9F%E5%A7%8B%E7%B1%BB%E5%9E%8B%E4%B8%8E%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B%E5%9C%A8%E8%87%AA%E5%A2%9E%E8%BF%90%E7%AE%97%E4%B8%8A%E7%9A%84%E5%8C%BA%E5%88%AB/</guid><description>在编码时，为了实现计数，我们可能会在for循环中对原始类型或者包装类型的值（对于包装类型是实例）进行自增，那么它们两种写法的区别如何？我们可以写出如下测试代码：
private static void primitive() { int i = 0; i++; System.out.println(i); } private static void wrapper() { Integer i = 0; i++; System.out.println(i); } 截图自字节码查看工具jclasslib
primitive方法：
wrapper方法：
可以看到包装类型的自增比原始类型复杂得多，应尽量避免这种情况。</description></item><item><title>🐚初识布隆过滤器</title><link>https://zenuo.github.io/posts/2019/07/01/%E5%88%9D%E8%AF%86%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/</link><pubDate>Mon, 01 Jul 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2019/07/01/%E5%88%9D%E8%AF%86%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/</guid><description>维基百科：布隆过滤器（英语：Bloom Filter）是1970年由布隆提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。
本文翻译自Probabilistic Data structures: Bloom filter
如果您有一个玻璃保护的书架，这将保护您的书籍免受灰尘和昆虫的侵害，但在您需要时，您将花费更多时间来阅读书籍。因为你首先需要滑动或打开玻璃然后才能拿到书。另一方面，如果它是一个开放的书架，这将使您更快地访问，但您将失去保护。同样，如果您按照其姓名的字典顺序组织图书，如果您知道图书的名称，则可以轻松搜索图书。但是，如果你的书架上有不同大小的盒子，你可以根据它们的大小来整理你的书籍，它看起来不错，但是你能赶紧找到一本书吗？我不这么认为。
数据结构和书架一样，您可以在其中组织数据。不同的数据结构将为您提供不同的设施和好处。要正确使用数据结构的功能和可访问性，您需要知道使用数据结构的权衡。
当主流数据结构如List，Map，Set，Tree等等主要用于实现关于数据是否存在的某些结果时，可能伴随着它们的出现次数等，概率数据结构（Probabilistic Data structures）将为您提供内存效率更快的结果是提供可能的结果而不是某个结果。现在使用这样的数据结构可能看起来不太直观，但我会在本文中试图说服这些类型的数据结构具有特定的用例，并且您可能会发现它们在某些情况下很有用。
在这篇文章中，我将讨论一种称为“布隆过滤器”的最流行的概率数据结构。
布隆过滤器 你知道哈希表是如何工作的吗？在简单数组或列表中插入新数据时，将插入此数据的索引不是从要插入的值确定的。这意味着&amp;rsquo;key(index)&amp;lsquo;和&amp;rsquo;value(data)&amp;lsquo;之间没有直接关系。因此，如果需要在数组中搜索值，则必须在所有索引中进行搜索。现在，在哈希表中，您可以通过散列值来确定键或索引。然后将此值放在列表中的该索引中。这意味着key是根据value确定的，每次你需要检查列表中是否存在该值时，只需对值进行散列并搜索该键即可。它非常快，并且需要以大O表示法进行O(1)搜索时间。
现在，让我们考虑一下你有一个庞大的弱密码列表，它存储在一些远程服务器上。由于尺寸的原因，无法在内存中一次加载它们。每次用户输入他/她的密码时，你想检查它是否是弱密码之一，如果是，你想给他/她一个警告，把它改成更强的密码。你能做什么？由于您已经拥有弱密码列表，您可以将它们存储在哈希表或类似的东西中，并且每次要匹配时，如果给定的密码有任何匹配，您可以检查它。匹配可能很快，但在磁盘上或通过远程服务器上的网络进行搜索的成本会使其变慢。不要忘记，您需要为每个用户提供的每个密码执行此操作。我们如何降低成本？
好吧，Bloom过滤器可以帮助我们。怎么样？在解释布隆过滤器的工作原理后，我将回答这个问题。
根据定义，Bloom过滤器可以检查值是“可能在集合中”还是“绝对不在集合中”。 可能和绝对不之间的细微差别在这里非常重要。这个“可能在集合中”正是它被称为概率的原因。使用智能词语意味着可能存在误报（false positive）（可能存在错误地认为该元素是阳性的情况）但是假阴性是不可能的。不要急躁，我们很快就会解释它究竟意味着什么。
布隆过滤器基本上由长度为m的位向量或位列表（仅包含0或1位值的列表）组成，最初所有值都设置为0，如下所示：
要将项添加到bloom过滤器，我们将其提供给k个不同的哈希函数，并在结果位置将这些位设置为1。如您所见，在哈希表中我们将使用单个哈希函数，因此只获得一个索引作为输出。但是在布隆过滤器的情况下，我们将使用多个哈希函数，这将为我们提供多个索引。
正如您在上面的示例中所看到的，对于给定的输入&amp;rsquo;geeks&amp;rsquo;，我们的3个散列函数将给出3个不同的输出 -1,4和7，我们已经标记了它们。
对于另一个输入&amp;rsquo;nerd&amp;rsquo;，哈希函数给出了3,4和5.您可能已经注意到索引'4&amp;rsquo;已经被先前的&amp;rsquo;geeks&amp;rsquo;输入标记。坚持你的想法，这一点很有趣，我们很快就会讨论它。
我们已经用两个输入填充了我们的位向量，现在我们可以检查它存在的值。我们怎么做？ 简单。就像我们在散列表中完成它一样。我们将使用我们的3个哈希函数对搜索输入进行哈希处理，并查看结果索引保持的内容。
因此，搜索&amp;rsquo;cat&amp;rsquo;，我们的哈希函数这次给我们1,3和7。我们可以看到所有索引都已标记为1.这意味着我们可以说，“也许&amp;rsquo;cat&amp;rsquo;已经插入我们的列表中”。但事实并非如此。出了什么问题？
实际上，没有出错。问题是，这是误报的情况。布隆过滤器告诉我们似乎之前可能插入了&amp;rsquo;cat&amp;rsquo;，因为索引应该已经被&amp;rsquo;cat&amp;rsquo;标记（尽管是其他不同的数据）。
那么，如果是这样的话，它有什么用呢？那么，让我们考虑一下&amp;rsquo;cat&amp;rsquo;是否会给我们输出1,6,7而不是1,3,7，那么会发生什么呢？我们可以看到，在3个索引中，6为'0&amp;rsquo;，这意味着它没有被任何先前的输入标记。这意味着很明显&amp;rsquo;猫&amp;rsquo;从未插入过，如果是的话，那么6就没有机会成为'0&amp;rsquo;，对吗？这就是如果数据不在列表中，布隆过滤器可以“肯定”地告诉它。
所以，简而言之：
如果我们搜索一个值并看到该值的任何散列索引为'0&amp;rsquo;，那么该值肯定不在列表中。 如果所有散列索引都为“1”，则“可能”搜索的值在列表中。 它开始有意义吗？有点可能吗？
很好，现在，回到我们之前谈到的&amp;rsquo;密码&amp;rsquo;示例。如果我们用这种类型的布隆过滤器实现我们的弱密码检查，你可以看到，最初，我们会用我们的密码列表标记我们的布隆过滤器，这将给我们一个位向量，其中一些索引标记为'1&amp;rsquo;而其他索引因为布隆过滤器的大小不会很大并且是固定大小，所以如果需要，它可以很容易地存储在存储器中，也可以存储在客户端。这就是为什么布隆过滤器非常节省空间的原因。在散列表需要基于输入数据的任意大小的情况下，布隆过滤器可以在固定大小下很好地工作。
因此，每次用户输入密码时，我们都会将其提供给我们的哈希函数，并根据我们的位向量进行检查。如果密码足够强，布隆过滤器将向我们显示密码肯定不在“弱密码列表”中，我们不必再进行任何查询。但是如果密码看起来很弱并且给我们一个“肯定的”（可能是误报）结果，我们会将它发送到我们的服务器并检查我们的实际列表以确认。
如您所见，大多数时候我们甚至不需要向我们的服务器发出请求或从磁盘读取以检查列表，这将显着提高应用程序的速度。如果我们不想在客户端存储位向量，我们仍然可以将其加载到服务器内存中，这至少可以节省一些磁盘查找时间。还要考虑一下，如果您的布隆过滤器误报率为1％（我们稍后会详细讨论错误率），这意味着在服务器或磁盘的昂贵往返中，只有1％的查询将是以虚假结果返回，其他99％不会徒劳无功。不错。
布隆过滤器操作 基本布隆过滤器支持两种操作：测试和添加。测试用于检查给定元素是否在集合中。添加只是向集合添加元素。
现在问题来了，根据我们到目前为止所讨论的内容，是否可以从布隆过滤器中删除项目？如果是，那怎么样？
休息2分钟，考虑一下解决方案。
现在我们要从中删除&amp;rsquo;geeks&amp;rsquo;。因此，如果我们从位向量中移除1,4,7，因为它们被&amp;rsquo;geeks&amp;rsquo;标记，并将它们转换为'0&amp;rsquo;，会发生什么？您可以很容易地看到，下次如果我们搜索&amp;rsquo;nerd&amp;rsquo;，因为索引'4&amp;rsquo;将显示'0&amp;rsquo;，它肯定会告诉我们&amp;rsquo;nerd&amp;rsquo;不在列表中，尽管它实际上是。这意味着在不引入假阴性的情况下无法移除。
那么，解决方案是什么？
解决方案是我们不能在这个简单的布隆过滤器中支持删除操作。但是如果我们真的需要具有移除功能，我们可以使用称为计数布隆过滤器的布隆过滤器的变体。这个想法很简单。我们将存储一个整数值，而不是存储单个位值，然后我们的位向量将是一个整数向量。这将增加尺寸并增加空间，为我们提供删除功能。我们不会在插入值时将位值标记为“1”，而是将整数值递增1.要检查元素是否存在，请检查散列元素后的相应索引是否大于0。
如果您很难理解“计数布隆过滤器”如何为我们提供“删除”功能，我建议您拿笔和纸张模拟我们的布隆过滤器作为计数过滤器，然后尝试删除它。希望你能轻松搞定。
布隆过滤器大小和散列函数的数量 您可能已经明白，如果布隆过滤器的大小太小，很快就会将所有位字段变为“1”，然后我们的布隆过滤器将为每个输入返回“误报”。因此，布隆过滤器的大小是一个非常重要的决定。较大的过滤器将具有较少的误报，并且较小的一个。因此，我们可以根据“误报误差率”调整我们的布隆过滤器以确定我们需要多少精确度。 另一个重要参数是“我们将使用多少哈希函数”。我们使用的哈希函数越多，布隆过滤器就越慢，填充越快。但是，如果我们的人数太少，我们可能会遭受太多误报。
从上图可以看出，增加散列函数k的数量将大大降低错误率p。</description></item><item><title>如何实现分布式锁</title><link>https://zenuo.github.io/posts/2019/06/23/%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/</link><pubDate>Sun, 23 Jun 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2019/06/23/%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/</guid><description>翻译自How to do distributed locking
作为本书的一部分，我在Redis发现了一种名为Redlock的算法，该算法声称在Redis上实现容错分布式锁（或者更确切地说，租用[1]），并且页面要求来自分布式系统地人的反馈。这个算法本能在我的脑海里引发了一些警钟，所以我花了些时间思考它，并写下这些笔记。
由于已经有超过10个独立的Redlock实现，我们不知道谁已经依赖这个算法，我认为值得公开分享我的笔记。我不会涉及Redis的其他方面，其中一些已经在其他地方被评论过。
在我详细介绍Redlock之前，请允许我说我非常喜欢Redis，过去我已成功将它用于生产中。我认为它非常适合您希望在服务器之间共享一些瞬态，近似，快速变化的数据的情况，以及如果您因某种原因偶尔丢失数据并不是什么大不了的地方。例如，一个好的用例是维护每个IP地址的请求计数器（用于速率限制）和每个用户ID的不同IP地址集（用于滥用检测）。
然而，Redis已经逐渐进入数据管理领域，这些领域具有更强的一致性和耐用性预期 - 这让我担心，因为这不是Redis的设计目标。 可以说，分布式锁定是其中一个领域。 让我们更详细地研究它。可以说，分布式锁定是其中一个领域。 让我们更详细地研究它。
你将锁定用于何处 锁定的目的是确保在可能尝试执行相同工作的多个节点中，只有一个实际执行它（至少一次只执行一次）。这项工作可能是将一些数据写入共享存储系统，执行某些计算，调用某些外部API等。 在较高的层次上，有两个原因可能导致您在分布式应用程序中需要锁定：效率或正确性。为了区分这些情况，您可以询问如果锁定失败会发生什么：
效率：锁定可以避免不必要地执行相同的工作两次（例如，一些昂贵的计算）。 如果锁定失败并且两个节点最终完成相同的工作，结果是成本略有增加（最终为AWS支付的费用比您原本要多5美分）或稍有不便（例如用户最终） 两次收到相同的电子邮件通知）。
正确性：采取锁定可防止并发进程踩到彼此的脚趾并弄乱系统状态。 如果锁定失败并且两个节点同时处理同一条数据，则结果是文件损坏，数据丢失，永久性不一致，给予患者的药物剂量错误或其他一些严重问题。
两者都是想要锁定的有效案例，但是你需要非常清楚你要处理的两个中的哪一个。
我将争辩说，如果您仅仅为了提高效率而使用锁，则不必承担Redlock的成本和复杂性，运行5台Redis服务器并检查大多数锁以获取锁。 您最好只使用一个Redis实例，可能在主要崩溃的情况下使用异步复制到辅助实例。
如果您使用单个Redis实例，当然如果Redis节点上的电源突然断电或者出现其他问题，您将丢弃一些锁。 但是如果你只是使用锁作为效率优化，并且崩溃不会经常发生，那没什么大不了的。 这个“没什么大不了的”情景是Redis闪耀的地方。 至少如果您依赖于单个Redis实例，那么每个看系统的人都很清楚锁是近似的，并且仅用于非关键目的。
另一方面，具有5个副本和多数表决权的Redlock算法乍一看，好像它适用于锁定对正确性很重要的情况。 我将在以下各节中论证它不适合这个目的。 对于本文的其余部分，我们将假设您的锁对于正确性很重要，并且如果两个不同的节点同时认为它们持有相同的锁，那么这是一个严重的错误。
使用锁保护资源 让我们暂时搁置Redlock的细节，并讨论如何使用分布式锁（与所使用的特定锁定算法无关）。 重要的是要记住，分布式系统中的锁不像多线程应用程序中的互斥锁。 这是一个更复杂的野兽，因为不同的节点和网络都可以以各种方式独立地失败。
例如，假设您有一个应用程序，其中客户端需要更新共享存储中的文件（例如HDFS或S3）。 客户端首先获取锁，然后读取文件，进行一些更改，将修改后的文件写回，最后释放锁。 该锁防止两个客户端同时执行此读 - 修改 - 写周期，这将导致更新丢失。 代码可能如下所示：
// THIS CODE IS BROKEN function writeData(filename, data) { var lock = lockService.</description></item><item><title>基于Wine使用QQ轻聊版</title><link>https://zenuo.github.io/fragments/2019-06-15-%E5%9F%BA%E4%BA%8Ewine%E4%BD%BF%E7%94%A8qq%E8%BD%BB%E8%81%8A%E7%89%88/</link><pubDate>Sat, 15 Jun 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2019-06-15-%E5%9F%BA%E4%BA%8Ewine%E4%BD%BF%E7%94%A8qq%E8%BD%BB%E8%81%8A%E7%89%88/</guid><description>wine是用于在其他操作系统上运行Windows应用程序的开源软件，我们可以用它来在Linux操作系统上使用QQ轻聊版客户端，满足日常的通信需求。当然，你还可以使用微信等客户端，操作流程与本文所述大同小异。
安装wine 执行命令：
# 安装wine和winetrick，我们用后者来安装一些依赖库 sudo pacman -S wine winetrick # 设置环境变量WINEARCH为win32 echo &amp;#34;\nexport WINEARCH=win32&amp;#34; &amp;gt;&amp;gt; ~/.profile &amp;amp;&amp;amp; source ~/.profile # 安装依赖库，此步骤需要下载一些文件，您可以设置http_proxy和https_proxy来使用代理，达到加速的目的 winetricks msxml3 gdiplus riched20 riched30 ie6 vcrun6 vcrun2005sp1 vcrun6sp6 # 追加配置到~/.wine/drive_c/windows/win.ini echo &amp;#34;\n[Desktop] menufontsize=13 messagefontsize=13 statusfontsize=13 IconTitleSize=13&amp;#34; &amp;gt;&amp;gt; ~/.wine/drive_c/windows/win.ini 创建文件font.reg：
REGEDIT4 [HKEY_LOCAL_MACHINE\Software\Microsoft\Windows NT\CurrentVersion\FontSubstitutes] &amp;#34;Arial&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;Arial CE,238&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;Arial CYR,204&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;Arial Greek,161&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;Arial TUR,162&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;Courier New&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;Courier New CE,238&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;Courier New CYR,204&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;Courier New Greek,161&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;Courier New TUR,162&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;FixedSys&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;Helv&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;Helvetica&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;MS Sans Serif&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;MS Shell Dlg&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;MS Shell Dlg 2&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;System&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;Tahoma&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;Times&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;Times New Roman CE,238&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;Times New Roman CYR,204&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;Times New Roman Greek,161&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;Times New Roman TUR,162&amp;#34;=&amp;#34;simsun&amp;#34; &amp;#34;Tms Rmn&amp;#34;=&amp;#34;simsun&amp;#34; 根据该文件修改注册表，执行：</description></item><item><title>MySQL随机查询一条记录</title><link>https://zenuo.github.io/fragments/2019-06-04-mysql%E9%9A%8F%E6%9C%BA%E6%9F%A5%E8%AF%A2%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95/</link><pubDate>Tue, 04 Jun 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2019-06-04-mysql%E9%9A%8F%E6%9C%BA%E6%9F%A5%E8%AF%A2%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95/</guid><description>表结构 CREATE TABLE `poet` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `dynasty` varchar(11) DEFAULT NULL, `author` text DEFAULT NULL, `paragraph` text DEFAULT NULL, `strains` text DEFAULT NULL, `title` text DEFAULT NULL, PRIMARY KEY (`id`), KEY `poet_title` (`title`(10)) USING BTREE, KEY `poet_dynasty` (`dynasty`) USING HASH, KEY `poet_author` (`author`(10)) USING BTREE ) ENGINE=InnoDB AUTO_INCREMENT=311862 DEFAULT CHARSET=utf8mb4; 查询语句 SELECT r1.</description></item><item><title>基于Nginx的反向代理</title><link>https://zenuo.github.io/fragments/2019-06-03-%E5%9F%BA%E4%BA%8Enginx%E7%9A%84%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86/</link><pubDate>Mon, 03 Jun 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2019-06-03-%E5%9F%BA%E4%BA%8Enginx%E7%9A%84%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86/</guid><description>配置文件如下：
user www; worker_processes 1; pid logs/nginx.pid; events { worker_connections 1024; use epoll; } http { log_format main &amp;#39;$remote_addr - $remote_user [$time_local] &amp;#34;$request&amp;#34; &amp;#39; &amp;#39;$status $body_bytes_sent &amp;#34;$http_referer&amp;#34; &amp;#39; &amp;#39;&amp;#34;$http_user_agent&amp;#34; &amp;#34;$google&amp;#34;&amp;#39;; access_log logs/access.log main; access_log off; include mime.types; default_type application/octet-stream; client_max_body_size 5M; client_body_buffer_size 256K; types_hash_max_size 2048; #设定DNS resolver [2001:4860:4860::8888] [2001:4860:4860::8844]; sendfile on; tcp_nopush on; keepalive_timeout 65; gzip on; proxy_connect_timeout 5; proxy_read_timeout 60; proxy_send_timeout 5; proxy_buffer_size 16k; proxy_buffers 4 64k; proxy_busy_buffers_size 128k; #proxy_temp_file_write_size 128k; #proxy_temp_path /var/nginx_cache/temp; #设定缓存的路径和其他参数，http://nginx.</description></item><item><title>Golang的交叉编译</title><link>https://zenuo.github.io/fragments/2019-05-27-golang%E7%9A%84%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91/</link><pubDate>Mon, 27 May 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2019-05-27-golang%E7%9A%84%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91/</guid><description>GOOS=${OS[$i]} \ GOARCH=${ARCH[$i]} \ go build -o $EXECUTABLE 合法的$GOOS和$GOARCH组合为：
$GOOS $GOARCH android arm darwin 386 darwin amd64 darwin arm darwin arm64 dragonfly amd64 freebsd 386 freebsd amd64 freebsd arm linux 386 linux amd64 linux arm linux arm64 linux ppc64 linux ppc64le linux mips linux mipsle linux mips64 linux mips64le linux s390x netbsd 386 netbsd amd64 netbsd arm openbsd 386 openbsd amd64 openbsd arm plan9 386 plan9 amd64 solaris amd64 windows 386 windows amd64 参考 Optional environment variables</description></item><item><title>MySQL查询数据并插入</title><link>https://zenuo.github.io/fragments/2019-05-27-mysql%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE%E5%B9%B6%E6%8F%92%E5%85%A5/</link><pubDate>Mon, 27 May 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2019-05-27-mysql%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE%E5%B9%B6%E6%8F%92%E5%85%A5/</guid><description>查询user表中的type不为1的数据并插入employee表：
-- 查询字段列表的字符串 select GROUP_CONCAT(COLUMN_NAME SEPARATOR &amp;#39;,&amp;#39;) from `information_schema`.`COLUMNS` WHERE TABLE_NAME = &amp;#39;employee&amp;#39;; -- 查询并插入 insert into employee(id,mobile,school_id,school_limit,nick_name,sex,picture_url,type,active,contact_mobile,password,salt,token,old_access_token_md5,forum_vest,forum_nickname,forum_head_portrait_url,status,create_time,update_time,delete_time) select id,mobile,school_id,school_limit,nick_name,sex,picture_url,type,active,contact_mobile,password,salt,token,old_access_token_md5,forum_vest,forum_nickname,forum_head_portrait_url,status,create_time,update_time,delete_time from user where type != 1;</description></item><item><title>缓存穿透、缓存击穿与缓存雪崩</title><link>https://zenuo.github.io/posts/2019/05/22/%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E4%B8%8E%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9/</link><pubDate>Wed, 22 May 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2019/05/22/%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E4%B8%8E%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9/</guid><description>翻译自3 major problems and solutions in the cache world
当前的IO设备远远不能满足互联网应用程序的大量读写请求。然后有一个缓存，使用内存的高速读写性能来应对大量的查询请求。但是，内存资源非常宝贵，将全部数据存储在内存中显然是不切实际的。因此，当前内存和IO的组合，内存只存储热点数据，而IO设备存储全部数据。缓存的设计包含很多技巧，不正确的设计会导致严重的后果。
1 缓存穿透（Cache penetration） 在大多数互联网应用程序中：
当业务系统发起某个查询请求时，它首先确定数据是否存在于缓存中; 如果有缓存，则直接返回数据; 如果缓存不存在，请再次查询数据库并返回数据。 在理解了上述过程之后，我们来谈谈缓存穿透。
1.1 是什么 当业务系统发起查询时，根据上面的过程，查询将首先进入缓存，因为缓存不存在，然后转到数据库进行查询。由于数据根本不存在，因此数据库也返回**null**。这是缓存穿透。
总结一下：业务系统访问根本不存在的数据称为缓存穿透。
1.2 危险是什么 如果查询请求中不存在大量数据，那么这些大量请求将落入数据库，数据库压力将急剧增加，这可能导致系统崩溃。 （你必须知道当前业务系统中最脆弱的是IO，有点它会在压力下崩溃，所以我们必须想办法保护它）。
1.3 为什么会发生 恶意攻击故意创建大量不存在的数据来请求我们的服务。由于缓存中不存在这些数据，因此大量请求会进入数据库，这可能导致数据库崩溃。 代码逻辑错误。这是程序员的锅，无话可说，必须在开发中避免！ 1.4 如何避免 以下是防止的两种方法：
1.4.1 缓存空数据 缓存穿透的原因是缓存中没有用于存储这些空数据的密钥，导致所有这些请求都到达数据库。
然后，我们可以稍微修改业务系统的代码，并将具有空数据库查询结果的密钥存储在缓存中。当再次发生对密钥的查询请求时，缓存直接返回**null**而不查询数据库。
1.4.2 布隆过滤器（BloomFilter） 它需要在缓存之前添加一个屏障，它存储当前数据库中存在的所有key。
当业务系统有查询请求时，首先转到BloomFilter以检查key是否存在。
如果它不存在，则表示数据库中不存在数据，因此不应检查缓存，并直接返回**null**。 如果存在，继续执行后续过程，首先查询缓存，如果没有缓存，则查询数据库。 1.4.3 比较两种方案 这两种解决方案都可以解决，但使用场景不同。
对于某些恶意攻击，查询的key通常不同，而数据窃贼更多。此时，第一个方案太多了。因为它需要存储所有空数据的key，并且这些恶意攻击的密钥通常是不同的，并且相同的密钥通常只被请求一次。因此，即使缓存了这些空数据的密钥，由于不再使用第二次，因此无法实现保护数据库的作用。
因此，对于空数据的key不同并且key重复请求的概率低的情况，应该选择第二方案。对于空数据的key数量有限且密钥重复请求的概率很高的情况，应选择第一种方案。
2 缓存击穿（Cache breakdown） 又称为热点数据集失效（Hotspot data set is invalid）</description></item><item><title>一个Shell管道的题目</title><link>https://zenuo.github.io/fragments/2019-05-17-%E4%B8%80%E4%B8%AAshell%E7%AE%A1%E9%81%93%E7%9A%84%E9%A2%98%E7%9B%AE/</link><pubDate>Fri, 17 May 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2019-05-17-%E4%B8%80%E4%B8%AAshell%E7%AE%A1%E9%81%93%E7%9A%84%E9%A2%98%E7%9B%AE/</guid><description>有如下a文件和b文件，请使用shell管道命令实现根据a文件中17:20~17:35期间doValidate的traceId找寻b文件中对应的日志信息：
$ cat a 2019-03-04 17:20:16 doValidate [traceId12] 2019-03-04 17:21:17 doValidate [traceId13] 2019-03-04 17:35:16 doValidate [traceId13] 2019-03-04 18:20:16 doValidate [traceId14] 2019-03-04 19:20:16 doValidate [traceId14] $ cat b 2019-03-04 17:20:17 xxxxxx2 [traceId12] 2019-03-04 17:21:18 xxxx22dd [traceId13] 2019-03-04 17:35:18 xxxx22dd [traceId13] 2019-03-04 18:20:17 xxxxed [traceId14] 2019-03-04 19:20:17 xxxsdsdfs [traceId14] 答案：
$ sed -n &amp;#39;/2019-03-04 17:20:*/,/2019-03-04 17:35:*/p&amp;#39; a|grep doValidate|awk &amp;#39;{print $4}&amp;#39;|uniq| xargs -I &amp;#39;{}&amp;#39; grep -F &amp;#39;{}&amp;#39; b 2019-03-04 17:20:17 xxxxxx2 [traceId12] 2019-03-04 17:21:18 xxxx22dd [traceId13] 2019-03-04 17:35:18 xxxx22dd [traceId13] 参考 BASH Programming - Introduction HOW-TO: Pipes Make xargs execute the command once for each line of input - Stack Overflow</description></item><item><title>About</title><link>https://zenuo.github.io/about/</link><pubDate>Wed, 15 May 2019 10:37:11 +0800</pubDate><guid>https://zenuo.github.io/about/</guid><description>一枚Ctrl-C和Ctrl-V工程师，面向猫猫🐱编程。</description></item><item><title>「勾勾」一个基于谷歌的搜索工具</title><link>https://zenuo.github.io/posts/2019/05/11/%E5%8B%BE%E5%8B%BE%E4%B8%80%E4%B8%AA%E5%9F%BA%E4%BA%8E%E8%B0%B7%E6%AD%8C%E7%9A%84%E6%90%9C%E7%B4%A2%E5%B7%A5%E5%85%B7/</link><pubDate>Sat, 11 May 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2019/05/11/%E5%8B%BE%E5%8B%BE%E4%B8%80%E4%B8%AA%E5%9F%BA%E4%BA%8E%E8%B0%B7%E6%AD%8C%E7%9A%84%E6%90%9C%E7%B4%A2%E5%B7%A5%E5%85%B7/</guid><description>是什么 「勾勾」是一个搜索工具，搜索结果基于谷歌，致力于「安全和简洁」的搜索体验。
安全 「勾勾」是一个在用户与谷歌之间的代理，谷歌无法得知用户的隐私（如UserAgent、Cookie等），也无法跟踪用户的结果点击
部署简单，基于JDK 11，仅需一台处于可以访问谷歌的网络的主机即可
简洁 精简（丑陋）到极致的Web前端 提供Web API，轻松地自定义搜索前端 如何使用 本程序通过网页、命令行和Web API三种方式提供服务。
网页 可访问实例体验
首页截图：
搜索页面截图：
命令行 请到Release页面下载可执行程序，并重命名为gogo-cli，放置到PATH路径下
$ gogo-cli github 1 截图如下：
Web API 搜索 $ curl -X GET -k &amp;#34;https://176.122.157.231:5000/api/search?q=github&amp;amp;p=1&amp;#34; { &amp;#34;key&amp;#34;: &amp;#34;github&amp;#34;, &amp;#34;page&amp;#34;: 1, &amp;#34;amount&amp;#34;: 223000000, &amp;#34;elapsed&amp;#34;: 0.43, &amp;#34;entries&amp;#34;: [ { &amp;#34;name&amp;#34;: &amp;#34;The world&amp;#39;s leading software development platform · GitHub&amp;#34;, &amp;#34;url&amp;#34;: &amp;#34;https://github.</description></item><item><title>一个成功的Git分支模型</title><link>https://zenuo.github.io/posts/2019/05/09/%E4%B8%80%E4%B8%AA%E6%88%90%E5%8A%9F%E7%9A%84git%E5%88%86%E6%94%AF%E6%A8%A1%E5%9E%8B/</link><pubDate>Thu, 09 May 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2019/05/09/%E4%B8%80%E4%B8%AA%E6%88%90%E5%8A%9F%E7%9A%84git%E5%88%86%E6%94%AF%E6%A8%A1%E5%9E%8B/</guid><description>翻译自A successful Git branching model
这篇文章介绍了一种开发模型，适用于工作或私人项目，只讨论分支策略和发布管理，不会讨论任何项目的细节。
为何是Git？ 你可以在这篇文章上查看关于Git与集中式源代码控制系统（centralized source code control systems）相比较的优缺点，讨论比较激烈。作为一个开发者，我更喜欢Git，而不是其他的所有工具。Git改变了开发者对于合并与分支的思考方式。从我之前所在的经典的CVS/Subversion世界来看，合并/分支一直被视为可怕的事情，每隔一段时间就会做一次，要小心合并冲突，它们会吃了你。
但在Git的世界里，这些动作极其廉价和简单，而且它们被视为日常工作流的核心部分。在CVS/Subversion的书中，首次介绍分支与合并是在最后一章（对于高级用户），然而Git的书中是在第三章（基础）。
由于其天生的简单性和重复性，分支和合并不再是一件令人害怕的事情。版本控制工具应该比其他任何东西更有助于分支/合并。
关于工具已经谈得足够了，让我们进入开发模型的讨论。我将介绍的模型基本上只是每个团队开发成员必须遵循的一组程序才能进入被管理的软件开发过程。
分散但集中 一个中心的真实的仓库，是我们用来与当前分支模型协作的。注：这个仓库仅仅被视为中心的一个，我们所用用户将其称作origin。
每个开发者从origin拉取（pull）和推送（push）。但是除了集中地推送拉取的关系，每个开发中也会从其他同事拉取变更来组成子团队，例如这在将工作进度永久推送到origin之前，两个或更多开发者协作开发一个新的功能很有用。在上图中，有三个子团队，分别是alice和bob、alice和david、david和clair。
从技术上讲，这意味着Alice定义了一个名为bob的远程（remote），指向Bob的仓库，反之亦然。
主要分支 在核心部分，这个开发模型极大程度上受下面的模型启发。中央仓库含有了两个具有无限生命时间的分支：
master develop origin的master分支应该对于所有Git用户都很熟悉。与master平行地，还存在一个称为develop的分支。
我们称origin/master是主要的分支，因为其HEAD的源代码总是在生产就绪状态。
我们称origin/develop是主要的分支，因为其HEAD的源代码总是在下一个发布版本的最新交付的开发变更。有时也被称作集成分支，是所有自动夜间构建的来源。
当develop分支的源代码达到稳定并且发布就绪的状态时，所有的变更应该被合并回到master，并被标记上一个发布编号。后面我们会讨论实现细节。
因此。每次将变更合并回master时，根据定义，这是i一个新的生产版本，对此应该非常严格。可以使用Git钩子脚本在每次有master提交时自动构建和推出软件到我们的生产服务器。
支持分支 在主分支master和develop之后，我们的开发模型还会用到一系列支持分支来用于团队成员之间的平行开发，使跟踪功能、准备生产发布和快速修复生产环境问题等。与主分支不同，这些分支通常的生命周期的有限的，因为它们最后会被删除。
我们用到的不同种类的分支是：
Feature分支 Release分支 Hotfix分支 这些分支的没一个都有特定的目的，并且必须遵守关于哪些分支必须是它们的合并目标的严格规则。我们下面开始讨论它们。
从技术角度来看，这些分支不是特殊的，分支类型取决与我们如何使用它们，它们是普通的Git分支。
Feature分支 可以从develop分支创建，可以合并到develop，分支命名惯例：除了master、develop、release-*或者hotfix-*之外的名称。
Feature分支（或者有时被称作Topic分支）通常备用做开发近期或者远期新的特征。当开始开发一个特征时，可能还不知道此特征将会合并到哪一个发布。一个feature分支的本质是在其对应特征开发过程中存在，但最终会被合并回develop（确定地将该特征添加到近期的发布中）或者被忽略（比如一个令人失望的实验的情况）。
创建一个feature分支 当开始一个新特征时，从develop分支创建：
$ git checkout -b myfeature develop Switched to a new branch &amp;#34;myfeature&amp;#34; 将一个完成的feature加入到develop $ git checkout develop Switched to branch &amp;#39;develop&amp;#39; $ git merge --no-ff myfeature Updating ea1b82a.</description></item><item><title>Git架构</title><link>https://zenuo.github.io/posts/2019/05/05/git%E6%9E%B6%E6%9E%84/</link><pubDate>Sun, 05 May 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2019/05/05/git%E6%9E%B6%E6%9E%84/</guid><description>翻译自The Architecture of Open Source Applications (Volume 2): Git
简述 Git允许许多协作者使用对等的存储库网络维护数字化工作体（通常但不限于代码），支持分布式工作流，允许工作体最终的交汇或暂时的分歧。本章将展示Git的各个方面如何实现，以及与其他版本控制系统（VCS）的区别。
来源 为了更好地理解Git的设计理念，有助于理解在Linux内核社区中启动Git项目的环境。
相较于当时的其他商业软件项目，Linux内核是不寻常的，因为提交者数量众多，贡献者参与度和现有代码库知识的差异很大。多年来，内核一直通过tarball和补丁进行维护，核心开发社区努力寻找满足其大部分需求的VCS。
Git是一个开源项目，源于2005年的需求和挫折。当时的Linux内行人代码库由两个VCS（BitKeeper和CVS）由不同的核心开发者管理。BitKeeper提供了与当前流行的开源VCS提供的VCS历史谱系不同的视图。
BitKeeper的制造商BitMover宣布将取消一些核心Linux内核开发人员的许可后几天，Linus Torvalds开始急于开发Git。他首先编写了一系列脚本来帮助他管理电子邮件补丁，以便一个接一个地应用。这个初始脚本集合的目的是能够快速中止合并，以便维护者可以修改code-mid-patch-stream中的代码库以手动合并，然后继续合并后续补丁。
从一开始，Torvalds对Git有一个哲学目标——成为反CVS加三个可用性设计目标：
支持分布式工作流，类似于BitKeeper启用的工作流。 提供防范内容损坏的保护措施。 提供高性能。 这些设计目标已在一定程度上完成和维护，正如我将试图通过剖析Git使用有向无环图（DAG）进行内容存储，头部参考指针，对象模型表示和远程协议来展示;最后Git如何跟踪树的合并。
尽管BitKeeper影响了Git的原始设计，但它以完全不同的方式实现，并允许更多的分布式加上仅限本地的工作流，这是BitKeeper无法实现的。 Monotone是一个开源的分布式VCS，始于2003年，可能是Git早期开发过程中的另一个灵感来源。
分布式版本控制系统提供了极大的工作流程灵活性，通常以简单性为代价分布式模型的特定优势包括：
为协作者提供脱机工作和递增提交的能力。 允许协作者确定他/她的工作何时可以分享。 在离线时提供协作者对存储库历史记录的访问权限。 允许将托管作业发布到多个存储库，可能会显示不同的分支或更改粒度。 在Git项目启动的时候，启动了另外三个开源分布式VCS项目。 （其中之一，Mercurial，在开源应用程序架构的第1卷中进行了讨论。）所有这些dVCS工具都提供了稍微不同的方法来实现高度灵活的工作流程，这些工作流程集中了VCS，直到它们无法直接处理。注意：Subversion有一个名为SVK的扩展，由不同的开发人员维护，以支持服务器到服务器的同步。
今天流行且积极维护的开源dVCS项目包括Bazaar，Darcs，Fossil，Git，Mercurial和Veracity。
版本控制系统设计 现在是退一步看看Git的替代VCS解决方案的好时机。了解他们的差异将使我们能够探索在开发Git时面临的架构选择。
版本控制系统通常有三个核心功能要求，即：
内容存储 跟踪内容更改（包括合并元数据的历史记录） 与协作者分发内容和历史记录 注意：上述第三个要求不是所有VCS的功能要求。
内容存储 用于在VCS世界中存储内容的最常见设计选择是基于增量的变更集，或者具有有向无环图（DAG）内容表示。
基于Delta的变更集封装了扁平内容的两个版本之间的差异，以及一些元数据。将内容表示为有向非循环图包含形成层次结构的对象，该层次结构将内容的文件系统树镜像为提交的快照（尽可能重用树内未更改的对象）。 Git使用不同类型的对象将内容存储为有向非循环图。本章后面的“对象数据库”部分描述了可以在Git存储库中形成DAG的不同类型的对象。
提交和合并历史 在历史和变更跟踪方面，大多数VCS软件使用以下方法之一：
线性历史 有向历史的有向无环图 Git再次使用DAG来存储其历史记录。每个提交都包含有关其祖先的元数据; Git中的提交可以有零个或多个（理论上无限制）父提交。例如，Git存储库中的第一个提交将具有零父项，而三向合并的结果将具有三个父项。 Git和Subversion及其线性历史祖先之间的另一个主要区别是它能够直接支持将记录大多数合并历史案例的分支。</description></item><item><title>关于Java语言的finally语句</title><link>https://zenuo.github.io/posts/2019/05/05/%E5%85%B3%E4%BA%8Ejava%E8%AF%AD%E8%A8%80%E7%9A%84finally%E8%AF%AD%E5%8F%A5/</link><pubDate>Sun, 05 May 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2019/05/05/%E5%85%B3%E4%BA%8Ejava%E8%AF%AD%E8%A8%80%E7%9A%84finally%E8%AF%AD%E5%8F%A5/</guid><description>是否一定执行、执行时机 一切要从这道题目说起：
Java语言中的finally语句（块）一定会被执行吗？是在try语句返回之前还是之后执行？
由于对这个问题的一知半解，当时只回答了前半个问题是的，一定会执行，没有回答后半个问题；笔试结束后查资料&amp;hellip;才发现事情没那么简单🤯
诚然，我的回答是错误的，以下两种情况的finally语句不会被执行：
try语句没有被执行；也说明了finally语句被执行的必要非充分条件是对应try语句被执行 try语句中有停止JVM的语句； 后半个问题的答案是之前，即在try语句返回之前执行，可通过一个简单的例子来说明：
jshell&amp;gt; String test1() { ...&amp;gt; System.out.println(&amp;#34;return statement&amp;#34;); ...&amp;gt; ...&amp;gt; return &amp;#34;after return&amp;#34;; ...&amp;gt; } | created method test1() jshell&amp;gt; String test2() { ...&amp;gt; try { ...&amp;gt; System.out.println(&amp;#34;try block&amp;#34;); ...&amp;gt; ...&amp;gt; return test1(); ...&amp;gt; } finally { ...&amp;gt; System.out.println(&amp;#34;finally block&amp;#34;); ...&amp;gt; } ...&amp;gt; } | created method test2() jshell&amp;gt; test2() try block return statement finally block $1 ==&amp;gt; &amp;#34;after return&amp;#34; 更详细地说明：执行return语句之后，再执行finally语句，再返回调用者；</description></item><item><title>可扩展的事件多路复用：epoll与kqueue</title><link>https://zenuo.github.io/posts/2019/05/01/%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E4%BA%8B%E4%BB%B6%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8epoll%E4%B8%8Ekqueue/</link><pubDate>Wed, 01 May 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2019/05/01/%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E4%BA%8B%E4%BB%B6%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8epoll%E4%B8%8Ekqueue/</guid><description>翻译自Scalable Event Multiplexing: epoll vs. kqueue
我比BSD更喜欢Linux，但我确实想在Linux中使用BSD的**kqueue**功能。
什么是事件多路复用（event multiplexing） 假设您有一个简单的Web服务器，并且当前有两个打开的连接（套接字）。当服务器从任一连接收到HTTP请求时，它应该向客户端发送HTTP响应。但是你不知道两个客户端中的哪一个会先发送消息，何时发送消息。BSD Socket API的阻塞行为意味着如果在一个连接上调用recv()，您将无法响应另一个连接上的请求。这是您需要**I/O多路复用（I/O multiplexing）**的地方。
I/O复用的一种简单方法是为每个连接提供一个进程/线程，以便在一个连接中阻塞不会影响其他连接。通过这种方式，您可以有效地将所有毛茸茸的调度/多路复用问题委托给OS内核。这种多线程架构带来（可以说）高成本。维护大量线程对于内核来说并非易事。为每个连接设置单独的堆栈会增加内存占用，从而降低CPU**缓存局部性（cache locality）**。
如何在没有线程连接的情况下实现I / O复用？您可以使用非阻塞套接字操作为每个连接执行繁忙等待轮询，但这太浪费了。我们需要知道的是哪个套接字准备好了。因此，操作系统内核在您的应用程序和内核之间提供了一个单独的通道，此通道会在您的某些套接字准备就绪时通知。这就是**select()/poll()的工作原理，基于就绪模型（readiness model）**。
回顾：select() select()和poll()在它们的工作方式上非常相似。让我快速回顾一下select()的样子。
select(int nfds, fd_set *r, fd_set *w, fd_set *e, struct timeval *timeout) 使用**select()，您的应用程序需要提供三个兴趣集（interest sets）r，w和e**。每个集都表示为文件描述符的位图。例如，如果您对从文件描述符6中读取感兴趣，则将**r的第六位设置为1。这个调用是阻塞的，直到感兴趣集中的一个或多个文件描述符准备就绪，这样您就可以对这些文件描述符执行没有阻塞的操作**。返回后，内核将覆盖位图以指定哪些文件描述符已准备就绪。
在可扩展性方面，我们可以找到四个问题：
这些位图的大小是固定的（FD_SETSIZE，通常为**1024**）。但是，有一些方法可以解决这个限制。 由于位图被内核覆盖，用户应用程序应该为每个调用重新填充兴趣集。 用户应用程序和内核应扫描每个调用的整个位图，以确定哪些文件描述符属于兴趣集和结果集。这对于结果集来说尤其低效，因为它们可能非常稀疏（即，在给定时间只有少数文件描述符准备好）。 内核应迭代整个兴趣集，以找出哪些文件描述符已准备好，再次针对每个调用。如果它们都没有准备就绪，则内核再次迭代以为每个套接字注册内部事件处理程序。 回顾：poll() poll()旨在解决其中的一些问题。
poll(struct pollfd *fds, int nfds, int timeout) struct pollfd { int fd; short events; short revents; } **poll()**不依赖于位图，而是依赖于文件描述符数组（因此解决了问题＃1）。通过为感兴趣（事件）和结果（revents）提供单独的字段，如果用户应用程序正确维护并重新使用该数组，则问题＃2也会得到解决。如果轮询分离数组而不是字段，问题＃3可能已经修复。最后一个问题是固有的，不可避免的，因为select()和poll()都是无状态的；内核不会在内部维护兴趣集。</description></item><item><title>MySQL中的并、差、交和除运算</title><link>https://zenuo.github.io/fragments/2019-04-26-mysql%E4%B8%AD%E7%9A%84%E5%B9%B6%E5%B7%AE%E4%BA%A4%E5%92%8C%E9%99%A4%E8%BF%90%E7%AE%97/</link><pubDate>Fri, 26 Apr 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2019-04-26-mysql%E4%B8%AD%E7%9A%84%E5%B9%B6%E5%B7%AE%E4%BA%A4%E5%92%8C%E9%99%A4%E8%BF%90%E7%AE%97/</guid><description>翻译自Union, Difference, Intersection, and Division in MySQL
构建数据库 -- 创建a和b表 CREATE TABLE a (x INT, y VARCHAR(5)); CREATE TABLE b (x INT, y VARCHAR(5)); -- 插入数据到a表 INSERT INTO a(x,y) VALUES(1,&amp;#39;A&amp;#39;); INSERT INTO a(x,y) VALUES(2,&amp;#39;B&amp;#39;); INSERT INTO a(x,y) VALUES(3,&amp;#39;C&amp;#39;); INSERT INTO a(x,y) VALUES(4,&amp;#39;D&amp;#39;); -- 插入数据到b表 INSERT INTO b(x,y) VALUES(1,&amp;#39;A&amp;#39;); INSERT INTO b(x,y) VALUES(3,&amp;#39;C&amp;#39;); SELECT * FROM a; +------+------+ | x | y | +------+------+ | 1 | A | | 2 | B | | 3 | C | | 4 | D | +------+------+ SELECT * FROM b; +------+------+ | x | y | +------+------+ | 1 | A | | 3 | C | +------+------+ 并集(Union) SELECT * FROM a UNION SELECT * FROM b; +------+------+ | x | y | +------+------+ | 1 | A | | 2 | B | | 3 | C | | 4 | D | +------+------+ SELECT * FROM a UNION ALL SELECT * FROM b; +------+------+ | x | y | +------+------+ | 1 | A | | 2 | B | | 3 | C | | 4 | D | | 1 | A | | 3 | C | +------+------+ 差集(Difference) SELECT * FROM a WHERE (x,y) NOT IN (SELECT * FROM b); +------+------+ | x | y | +------+------+ | 2 | B | | 4 | D | +------+------+ SELECT * FROM a WHERE NOT EXISTS (SELECT * FROM b WHERE b.</description></item><item><title>事务隔离等级提交读与可重复读的区别</title><link>https://zenuo.github.io/posts/2019/04/20/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%AD%89%E7%BA%A7%E6%8F%90%E4%BA%A4%E8%AF%BB%E4%B8%8E%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E7%9A%84%E5%8C%BA%E5%88%AB/</link><pubDate>Sat, 20 Apr 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2019/04/20/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%AD%89%E7%BA%A7%E6%8F%90%E4%BA%A4%E8%AF%BB%E4%B8%8E%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E7%9A%84%E5%8C%BA%E5%88%AB/</guid><description>翻译自Differences between READ-COMMITTED and REPEATABLE-READ transaction isolation levels
作为Percona的讲师，我有时会被问及已提交读和可重复读事务隔离级别之间的区别，它们存在一些差异，但都与锁定有关。
额外锁定 (非间隙锁定) 请记住，InnoDB实际上锁定了索引条目（index entries），而不是行（rows），这很重要。在执行语句期间，InnoDB必须锁定它遍历的索引中的每个条目，以查找它正在修改的行。他必须这样做以防止死锁并保持隔离级别。
若执行一句未被良好索引的UPDATE语句，则会锁定很多行：
update employees set store_id = 0 where store_id = 1; ---TRANSACTION 1EAB04, ACTIVE 7 sec -- 633 lock struct(s), &amp;lt;strong&amp;gt;heap size 96696&amp;lt;/strong&amp;gt;, 218786 row lock(s), undo log entries 1 -- MySQL thread id 4, OS thread handle 0x7f8dfc35d700, query id 47 localhost root -- show engine innodb status 在employee表中，列store_id没有被索引。请注意，UPDATE已经完成运行（我们现在正在运行SHOW ENGINE &amp;hellip;）但是我们持有218786个行锁并且只有一个撤销条目（undo entry）。这意味着只有一行被更改，但我们仍然持有额外的锁。堆大小表示已为锁分配的内存量。</description></item><item><title>class-visualizer——一个查看类关系的图形化工具</title><link>https://zenuo.github.io/fragments/2019-04-19-class-visualizer%E4%B8%80%E4%B8%AA%E6%9F%A5%E7%9C%8B%E7%B1%BB%E5%85%B3%E7%B3%BB%E7%9A%84%E5%9B%BE%E5%BD%A2%E5%8C%96%E5%B7%A5%E5%85%B7/</link><pubDate>Fri, 19 Apr 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2019-04-19-class-visualizer%E4%B8%80%E4%B8%AA%E6%9F%A5%E7%9C%8B%E7%B1%BB%E5%85%B3%E7%B3%BB%E7%9A%84%E5%9B%BE%E5%BD%A2%E5%8C%96%E5%B7%A5%E5%85%B7/</guid><description>本人使用的IntelliJ IDEA Community Edition没有Class Diagram功能，在逛stackoverflow时，发现了工具Class Visualizer，通过加载Jar包的形式绘制类关系图表，在阅读开源项目源代码时能提供帮助，下图是io.netty.channel.Channel的关系图：
下图是其类图：
真香😋
参考：
Class diagrams - Help | IntelliJ IDEA - JetBrains Sketch It! - Plugins | JetBrains Application to generate Java class hierarchy diagram</description></item><item><title>树莓派电源最大化</title><link>https://zenuo.github.io/fragments/2019-04-19-%E6%A0%91%E8%8E%93%E6%B4%BE%E7%94%B5%E6%BA%90%E6%9C%80%E5%A4%A7%E5%8C%96/</link><pubDate>Fri, 19 Apr 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2019-04-19-%E6%A0%91%E8%8E%93%E6%B4%BE%E7%94%B5%E6%BA%90%E6%9C%80%E5%A4%A7%E5%8C%96/</guid><description>在将无电源的USB移动硬盘连接在树莓派上时，会发现无法正常驱动，在移动硬盘无故障的前提下是树莓派默认没有允许USB电流最大，经过谷歌🦴，找到如下解决办法：
在文件/boot/config.txt中添加：
max_usb_current=1 重启即可。
参考：
https://www.raspberrypi.org/forums/viewtopic.php?t=105502</description></item><item><title>使用tcpdump抓包</title><link>https://zenuo.github.io/fragments/2019-03-10-%E4%BD%BF%E7%94%A8tcpdump%E6%8A%93%E5%8C%85/</link><pubDate>Sun, 10 Mar 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2019-03-10-%E4%BD%BF%E7%94%A8tcpdump%E6%8A%93%E5%8C%85/</guid><description>在开发网络程序时，可以使用WireShark等工具在本地抓包，但往往本地无法模拟生产环境的网络状况，所以我们有在服务器上抓包的需求；tcpdump可以胜任这个工作，首先你可以使用包管理器直接安装它，也可以自己构建，安装完成后，可以根据需求使用其过滤语法确定抓取的流量的范围，例如：在所有网络接口上抓取端口是5070的流量，并且保存到文件~/5070.pcap：
$ sudo tcpdump -nvAx -i any &amp;#39;port 5070&amp;#39; -w ~/5070.pcap 将文件拷贝到本地后，可使用WireShark打开，如图：
参考 Let&amp;rsquo;s learn tcpdump! by Julia Evans Tcpdump filters by Marios Iliofotou</description></item><item><title>实践java.util.concurrent.TransferQueue</title><link>https://zenuo.github.io/posts/2019/03/10/%E5%AE%9E%E8%B7%B5java.util.concurrent.transferqueue/</link><pubDate>Sun, 10 Mar 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2019/03/10/%E5%AE%9E%E8%B7%B5java.util.concurrent.transferqueue/</guid><description>综述 java.util.concurrent.TransferQueue是Java Collections Framework的成员之一，允许我们根据生产者-消费者模式创建程序，并协调从生产者传递给消费者的消息，其JDK内的实现是java.util.concurrent.LinkedTransferQueue。
实现实际上类似于BlockingQueue - 但是为我们提供了实现backpressure形式的新功能。这意味着，当生产者使用transfer()方法向使用者发送消息时，生产者将保持阻塞状态，直到消息被消耗为止（tryTransfer()是非阻塞的）。
可以通过hasWaitingConsumer()查询是否有任何消费者在等待，与peek()操作相反。
一个生产者——无消费者 首先，设计一个生产者来测试transfer()方法 - 预期是生产者将被阻塞，直到消费者使用take()方法从队列接收消息。
class Producer implements Runnable { private TransferQueue&amp;lt;String&amp;gt; transferQueue; private String name; private Integer numberOfMessagesToProduce; public AtomicInteger numberOfProducedMessages = new AtomicInteger(); @Override public void run() { for (int i = 0; i &amp;lt; numberOfMessagesToProduce; i++) { try { boolean added = transferQueue.</description></item><item><title>MySQL测试表中是否存在行的最佳方法</title><link>https://zenuo.github.io/fragments/2019-02-22-mysql%E6%B5%8B%E8%AF%95%E8%A1%A8%E4%B8%AD%E6%98%AF%E5%90%A6%E5%AD%98%E5%9C%A8%E8%A1%8C%E7%9A%84%E6%9C%80%E4%BD%B3%E6%96%B9%E6%B3%95/</link><pubDate>Fri, 22 Feb 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2019-02-22-mysql%E6%B5%8B%E8%AF%95%E8%A1%A8%E4%B8%AD%E6%98%AF%E5%90%A6%E5%AD%98%E5%9C%A8%E8%A1%8C%E7%9A%84%E6%9C%80%E4%BD%B3%E6%96%B9%E6%B3%95/</guid><description>SELECT EXISTS( SELECT id FROM funds WHERE user_id = #{userId} AND school_id = #{schoolId} LIMIT 1) 参考 best-way-to-test-if-a-row-exists-in-a-mysql-table</description></item><item><title>Nginx架构</title><link>https://zenuo.github.io/posts/2019/01/31/nginx%E6%9E%B6%E6%9E%84/</link><pubDate>Thu, 31 Jan 2019 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2019/01/31/nginx%E6%9E%B6%E6%9E%84/</guid><description>翻译自The Architecture of Open Source Applications (Volume 2): nginx
nginx（读作&amp;quot;engine x&amp;quot;）是由Igor Sysoev（一名俄罗斯软件工程师）开发的一款自由开源的Web服务器程序。自2004年公开发布以来，nginx一直专注于高性能、高并发行和低内存使用。负载均衡、缓存和访问及带宽控制等Web服务器功能之上的其他功能，以及有效集成各种应用程序的能力，都帮助使nginx成为现代架构的不错选择。当前nginx是互联网上第二受欢迎的开源Web服务器。
1 为何高并发性是重要的 如今，互联网如此广泛和无处不在，很难想象它不是十年前我们所知道的那样。从简单的HTML生成可点击文本，基于NCSA，然后是Apache Web服务器，到全球超过20亿用户使用的在线通信媒体，它已经有了长足发展。随着永久连接的个人电脑、移动设备和平板电脑的激增，互联网领域正在迅速变化，整个经济已经成为数字连线。在线服务变得更加精细，明显偏向即使可用的实时信息和娱乐。运行在线业务的安全方面也发生了重大变化。因此，现在的网站比以前复杂得多，并且通常需要更多的工程努力才能具有健壮性和可拓展性。
一个网站架构的最大的变化之一经常是并发性。从Web服务开始以来，并发级别一直在不断增长。一个流行网站同时服务数十万甚至数百万用户的情况并不罕见。十年前，并发的主要原因是慢客户端（slow clients）——具有ADSL或拨号连接的用户。如今，并发是由于移动客户端和较新的应用程序体系架构的组合引起的，这些体系结构通常基于持久连接，以支持客户端实时更新新闻、推文和朋友订阅源等。另一个因素是现代浏览器的行为改变——它可以打开四到六个与网站的同时连接，以提高页面加载速度。
为了说明慢客户端的问题，想象一个简单的基于Apache Web服务器，它产生一个相对较短的100 KB响应——一个带有文本或图像的网页。生成或健讼此页面只需要几分之一秒，但需要10秒才能将其传输到带宽为80 kbps（10KB/s）的客户端。从本质上讲，Web服务器会相对快都地提取100KB的内容，然后在释放连接之前，它将忙于将内容缓慢地（10秒）发送到客户端。假设有1000个同时连接的客户，他们呢请求了类似的内容。若每个客户端分配1MB的额外内存，则会产生1000 MB（约1 GB）的额外内存，专门用于为1000个用户端提供100 KB的内容。实际上，基于Apache的典型Web服务器通常为每个连接分配超过1 MB的额外内存，令人遗憾的是，数十kbps仍然是移动通信的有效速度。虽然在某种程度上通过增加操作系统内核socket缓冲区大小来改善向慢速客户端发送内懂的情况，但这不是解决该问题的一般办法，并且可能具有不良副作用。
为了避免建立新HTTP连接的延时，客户端与服务端保持连接（persistent connection），并发处理的问题更加明显，Web服务器为每个已连接的客户端分配一定数量的内存。
因此，为了处理与增长的受众相关的增加的工作量以及因此更高的并发水平——并且能够持续这样做——网站应该基于许多非常有效的构建块（building block）。硬件（CPU、内存、磁盘等）、网络能力、数据存储架构显然与Web服务器接受和处理客户端连接同等重要。因此，Web服务器应该能够随着并发连接和请求数量的增加而非线性地拓展。
Apache不合适吗？ Apache这款web服务器程序在很大程度上仍然主宰着互联网，它起源于20世纪90年代初。最初，它的架构与当时存在的操作系统和硬件相匹配，其中网站通常是运行单个Apache实例的独立物理服务器。2000年代初，轻松复制独立的Web服务器模型显然不足以满足不断增长的Web服务需求。尽管Apache为未来的开发提供了坚实的基础，但它的架构是为每个新连接生成自己的副本，这不适合网站的非线性可伸缩性。最终，Apache成为了一个通用的Web服务器，专注于拥有许多不同的功能，各种第三方拓展，以及几乎任何类型Web应用程序开发的普遍适用性。然后，没有任何代价地在单个软件中拥有如此丰富和通用的工具组合的缺点是可伸缩性较差，因为每个连接都会增加CPU和内存的使用量。
因此，当服务器硬件、操作系统和网络资源不再成为网站增加的主要限制时，全球的Web开发人员开始寻找更有效的运行Web服务器的方法。大约十年前，著名软件工程师Daniel Kegel宣称：Web服务器是时候同时处理一万个客户端的时候了，并预测了我们现在所谓互联网云服务的东西。Kegel的C10K清单激发了许多尝试来解决Web服务器优化问题，同时处理大量客户端，而nginx成为最成功的之一。
旨在解决10,000个同时连接的C10K问题，nginx在编写时考虑了不同的体系结构——一个更适合并发连接数和每秒请求数的非线性可伸缩性。nginx是基于事件的（event-based），并不遵循Apache为每个网页请求生成新进程或线程的风格，以实现即使负载增加，CPU和内存适用仍然可控。nginx现在可以在一台典型主机上处理数万个并发连接。
nginx首个版本发布时，作为静态内容（如HTML、CSS、JavaScript和图片）服务器与Apache一起部署，分担基于Apache的应用服务器的并发和延时。在开发过程中，nginx通过适用FastCGI，uSWGI或SCGI协议以及分布式内存对象缓存系统（如memcached）增加了与应用程序的集成，其他有用的功能也被开发出来，如具有负载均衡和缓存反向代理（reverse proxy）等等。这些附加功能使nginx成为有效的工具组合，可构建可拓展的Web基础架构。
2012年2月，Apache 2.4.x分支公开发布，尽管其最新版本添加了新的多处理核心模块，以及新的指向增强可伸缩性和性能的代理模块，现在谈论它与纯事件驱动的Web服务器谁在性能、并发和资源利用等方面更好还为时尚早。非常高兴能看到Apache应用服务器伸缩性能更好，不过，可能缓解在典型nginx加Apache的Web配置中仍然没解决的问题。
使用nginx有更多的优势吗？ 以高性能和高效率处理高并发始终是部署nginx的关键优势，但现在有更多有趣的好处。
在最近几年，Web架构师已经接受了将其应用程序基础设施与Web服务器解耦和分离的想法。然而，以前以LAMP（Linux，Apache，MySQL，PHP，Python或Perl）为基础的网站形式，现在可能不仅仅是一个基于LEMP的（E&amp;rsquo;代表&amp;rsquo;Engine x&amp;rsquo;）但是，越来越多的做法是将Web服务器推向基础设施的边缘，并以不同的方式围绕它集成相同或经过改进的应用程序和数据库工具集。
nginx很适合这些，因为它提供了方便处理并发、延迟，SSL，静态内容，压缩和缓存，连接和请求限制，甚至来自应用程序的HTTP媒体流所需的关键功能层到更有效的边缘Web服务器层，它还允许直接与memcached / Redis或其他“NoSQL”解决方案集成，以在为大量并发用户提供服务时提高性能。
nginx代码库是原创的，完全是用C编程语言从头开始编写的。 nginx已经移植到许多架构和操作系统，包括Linux，FreeBSD，Solaris，Mac OS X，AIX和Microsoft Windows。 nginx有自己的库，其标准模块除了zlib，PCRE和OpenSSL之外不会超出系统的C库，zlib，PCRE和OpenSSL可以选择从构建中排除，如果不需要或者由于潜在的许可证冲突。</description></item><item><title>Linux的TCP backlog如何工作</title><link>https://zenuo.github.io/posts/2018/11/18/linux%E7%9A%84tcp-backlog%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C/</link><pubDate>Sun, 18 Nov 2018 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2018/11/18/linux%E7%9A%84tcp-backlog%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C/</guid><description>本文翻译自How TCP backlog works in Linux。
当某个应用程序使用listen系统调用将一个socket置为LISTEN状态时，需要为这个socket设置参数backlog，该参数通常被描述为传入（incoming）连接队列的数量限制。
因为TCP使用三步握手（3-way handshake），在一个传入的连接在到达ESTABLISHED状态之前必须经过中间（intermediate）状态SYN RECEIVED，并且可由accept系统调用返回到应用程序（请参阅上面复制的TCP状态图）。这意味着TCP/IP堆栈有两个选项来实现LISTEN状态的socket的积压队列（backlog queue）：
使用大小由listen系统调用的backlog参数决定的单队列实现。当某个connection接收SYN分组时，它会发回SYN/ACK分组并将连接入列；当接收到相应的ACK分组时，连接将其状态改变为ESTABLISHED并且有资格切换到应用程序。也就是说，队列中包含两种不同状态——SYN RECEIVED和ESTABLISHED，只有后一种状态的connection才能通过accept系统调用返回给应用程序。
使用一个SYN队列（未完成的连接队列）和一个accept队列（已完成的连接队列）。状态SYN RECEIVED中的connection被添加到SYN队列中，并且当它们的状态变为ESTABLISHED时，即当接收到3次握手中的ACK分组时，移动到accept队列。顾名思义，accept系统调用然后只是为了消费（consume）来自accept队列的连接而实现。在这种情况下，listen系统调用的backlog参数确定accept队列的大小。
从历史上看，BSD派生的TCP实现使用第一种选项，意味着当达到最大backlog时，系统将不再发回SYN/ACK分组以响应SYN分组。通常，TCP实现只会丢弃SYN分组（而不是响应RST分组），以便客户端重试。这是W. Richard Stevens的经典教材TCP/IP详解 卷3的第14.5节listen Backlog Queue描述的内容。
请注意，Stevens实际上解释了BSD实现确实使用了两个单独的队列，但它们表现为单个队列，其固定的最大大小由backlog参数确定（但不一定完全等于），即BSD在逻辑上表现如第一个选项所述：
队列限制适用于[&amp;hellip;]不完整连接队列上的条目数和[&amp;hellip;]已完成连接队列[&amp;hellip;]上的条目数之和。
在Linux上，事情是不同的，如listen系统调用的手册页中所述：
Linux 2.2修改了TCP socket的backlog参数的行为。现在它指定了等待被accept的完全建立的套接字的队列长度，而不是未完成的连接请求的数量。可以在文件/proc/sys/net/ipv4/tcp_max_syn_backlog中设置未完成的socket队列的长度。
这意味着当前的Linux版本使用具有两个不同队列的第二个选项：具有由系统范围设置指定的大小的SYN队列和具有由应用程序指定的大小的accept队列。
现在有趣的问题是，如果接受队列已满并且需要将连接从SYN队列移动到接受队列，即当接收到3次握手的ACK分组时，这种实现如何表现。这种情况由net/ipv4/tcp_minisocks.c中的tcp_check_req函数处理，相关代码如下：
child = inet_csk(sk)-&amp;gt;icsk_af_ops-&amp;gt;syn_recv_sock(sk, skb, req, NULL); if (child == NULL) goto listen_overflow; 对于IPv4，第一行代码实际调用net/ipv4/tcp_ipv4.c中的tcp_v4_syn_recv_sock函数，包含以下代码：
if (sk_acceptq_is_full(sk)) goto exit_overflow; 此处的代码对accept队列进行了check。exit_overflow标签之后的代码将执行一些清理，更新/proc/net/netstat中的ListenOverflows和ListenDrops统计信息，然后返回NULL。这将引发tcp_check_req函数中的listen_overflow代码的执行：
listen_overflow: if (!</description></item><item><title>浅谈Java对函数式编程的支持</title><link>https://zenuo.github.io/posts/2018/11/09/%E6%B5%85%E8%B0%88java%E5%AF%B9%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%E7%9A%84%E6%94%AF%E6%8C%81/</link><pubDate>Fri, 09 Nov 2018 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2018/11/09/%E6%B5%85%E8%B0%88java%E5%AF%B9%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%E7%9A%84%E6%94%AF%E6%8C%81/</guid><description>此篇文章主要基于对java.util.function包的javadoc翻译，谈谈Java对函数式编程的支持，亦有助于学习Stram API。
1 什么是函数式编程？ 在计算机科学中，函数式编程（Functional programing）是一种将计算（Computation）视为数学函数的评估，避免改变状态（changing-state）和可变数据（mutable data）的编程范式（一种构建计算机程序结构和元素的方式）。
In computer science, functional programming is a programming paradigm—a style of building the structure and elements of computer programs—that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data. 摘自维基百科
2 java.util.function包 Java自1.8添加java.util.function包，以支持函数式编程；其将计算抽象为函数式接口（Functional interface），为lambda表达式（lambda expressions）和方法引用（method references）提供了目标类型。每一个函数式接口含有单个抽象方法，称为该函数式接口的函数式方法（functional method），lambda表达式的参数和返回类型与之匹配或适应。在多种上下文中，函数式接口可提供一个目标类型，如赋值、方法调用和转型上下文：
// 赋值上下文 Predicate&amp;lt;String&amp;gt; p = String::isEmpty; // 方法调用上下文 stream.</description></item><item><title>基于OpenCV的图像缩放脚本</title><link>https://zenuo.github.io/fragments/2018-10-14-%E5%9F%BA%E4%BA%8Eopencv%E7%9A%84%E5%9B%BE%E5%83%8F%E7%BC%A9%E6%94%BE%E8%84%9A%E6%9C%AC/</link><pubDate>Sun, 14 Oct 2018 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2018-10-14-%E5%9F%BA%E4%BA%8Eopencv%E7%9A%84%E5%9B%BE%E5%83%8F%E7%BC%A9%E6%94%BE%E8%84%9A%E6%9C%AC/</guid><description>1 OpenCV是什么 摘自维基百科的OpenCV词条：
OpenCV (Open Source Computer Vision) is a library of programming functions mainly aimed at real-time computer vision. Originally developed by Intel, it was later supported by Willow Garage then Itseez (which was later acquired by Intel). The library is cross-platform and free for use under the open-source BSD license.</description></item><item><title>Timsort 你从未听说过的最快的排序算法</title><link>https://zenuo.github.io/posts/2018/08/16/timsort-%E4%BD%A0%E4%BB%8E%E6%9C%AA%E5%90%AC%E8%AF%B4%E8%BF%87%E7%9A%84%E6%9C%80%E5%BF%AB%E7%9A%84%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/</link><pubDate>Thu, 16 Aug 2018 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2018/08/16/timsort-%E4%BD%A0%E4%BB%8E%E6%9C%AA%E5%90%AC%E8%AF%B4%E8%BF%87%E7%9A%84%E6%9C%80%E5%BF%AB%E7%9A%84%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/</guid><description>本文翻译自Timsort — the fastest sorting algorithm you’ve never heard of，如侵必删。
Timsort: 一个非常快速的，时间复杂度O(n log n)，为真实世界构建的稳定排序算法——而不是学术界。
Timsort是一个对于真实世界数据具有高效率的、不是在学术实验室中发表的排序算法，由Tim Peters在2001年为Python编程语言创造。它试图首先分析需要排序的列表，然后基于分析选择一种方法。
从发表至今，该算法已经被用作Python, Java, Android(TM)和GNU Octave用作默认排序算法。
Timsort的大O标记法(big O notation)是O(n log n).学习大O标记法，请移步至此。
Timsort的排序时间与归并排序相同，后者比您可能知道的其他大多数排序算法要快。正如你很快将要看到的，Timsort实际上使用了插入排序和归并排序。
Peters将Timsort设计为使用真实世界的数据中已经有序的元素。它将这些已经有序的元素称作“自然的运行(natural runs)”。它迭代整个数据，将元素收集到多个run中，同时将多个run合并到一个。
列表长度小于64 若列表尝试排序其内部少于64个元素时，Timsort将会执行一个插入排序。
插入排序是一种简单的排序，对于小型列表效率最高。在较大的列表中确实比较慢，但在小型列表中却非常快。插入排序的概念如下：
逐个查看元素 通过将元素插入正确的位置来构建排序列表 这是一个跟踪列表，显示了[34, 10, 64, 51, 32, 21]如何进行插入排序：
在这个例子中，我们将一个新排序的元素插入一个新的子数组，该子数组从数组的开头开始。
这是一个显示插入排序的GIF：
关于run的细节 如果列表大于64个元素，则算法将首先通过列表查找严格增加或减少的部分。如果某个部分正在减少，它将该部分反转。因此，如果运行正在减少，它将看起来像这样（运行以粗体显示）：
若不减少，则看起来像这样：
minrun是基于数组大小确定的。该算法选择它，以便大多数在随机数组中运行的长度为minrun。当运行次数等于或略小于2的幂时，合并2个数组会更高效。Timsort选择minrun来保持使minrun的长度不大于2的幂。该算法选择范围为32到64（包括32和64）的minrun，使得原始数组的长度除以minrun时等于或略小于2的幂。
如果某个run的长度小于minrun，则计算两者差值，使用此新数字，您可以在run的头部获取许多元素并执行插入排序以创建一个新的run。
因此，如果minrun为63并且run的长度为33，则执行63-33 = 30，然后从run的末尾前抓取30个元素，这是来自run[33]的30个元素然后执行用于创建新run的插入排序。</description></item><item><title>在树莓派上安装Arch Linux ARM</title><link>https://zenuo.github.io/fragments/2018-08-08-%E5%9C%A8%E6%A0%91%E8%8E%93%E6%B4%BE%E4%B8%8A%E5%AE%89%E8%A3%85arch-linux-arm/</link><pubDate>Wed, 08 Aug 2018 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2018-08-08-%E5%9C%A8%E6%A0%91%E8%8E%93%E6%B4%BE%E4%B8%8A%E5%AE%89%E8%A3%85arch-linux-arm/</guid><description>来自https://archlinuxarm.org/platforms/armv6/raspberry-pi
请将下面指令中的sdX替换为SD卡在您电脑中的设备名称。
使用fdisk为SD卡分区: # fdisk /dev/sdX 在fdisk的提示符中，删除原有的分区，然后新建分区： 按o，清除此设备上所有的分区； 按p来列出所有分区，当前设备应该没有任何分区存在； 按n，然后按p（主分区），然后按1（设备上第一个分区），然后按回车键接受默认的第一个扇区，然后为最后一个扇区输入+100M； 按t，然后按c设置第一个分区类型为W95 FAT32(LBA)； 按n，然后按p（主分区），按2（设备上第二个分区），然后按两次回车来接受默认的第一个和最后一个扇区； 按w，写入分区表后退出； 创建FAT文件系统并装载 # mkfs.vfat /dev/sdX1 # mkdir boot # mount /dev/sdX1 boot 创建ext4文件系统并装载: # mkfs.ext4 /dev/sdX2 # mkdir root # mount /dev/sdX2 root 下载系统镜像文件，并且提取到root文件系统（注，作为root用户，而不是通过sudo）: # wget http://os.archlinuxarm.org/os/ArchLinuxARM-rpi-latest.tar.gz # bsdtar -xpf ArchLinuxARM-rpi-latest.tar.gz -C root # sync # 此命令比较耗时 移动boot文件到第一个分区： # mv root/boot/* boot 卸载两个分区 # umount boot root 将SD卡插入到树莓派，连接以太网，并且连接电源 使用串行控制台(serial console)或者SSH连接 使用默认用户alarm，密码alarm root用户的密码默认是root 注：为了安全，请强化用户密码</description></item><item><title>什么是数学-Richard-Courant</title><link>https://zenuo.github.io/posts/2018/06/24/%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E5%AD%A6-richard-courant/</link><pubDate>Sun, 24 Jun 2018 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2018/06/24/%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E5%AD%A6-richard-courant/</guid><description>摘自什么是数学：对思想和方法的基本研究
**数学，作为人类思维的表达形式，反映了人们积极进取的意志、缜密周详的推理以及对完美境界的追求。**它的基本要素是：逻辑和直观、分析和构作、一般性和个别性。虽然不同的传统可以强调不同的侧面，然而正是这些互相对立的力量的相互作用以及它们综合起来的努力才构成了数学科学的生命、用途和它的崇高价值。
毫无疑问，一切数学的发展在心理上都或多或少地是基于实际的。但是理论一旦在实际的需要中出现，就不可避免地会使它自身获得发展的动力，并超越出直接实用的局限。这种从应用科学到理论科学的发展趋势，不仅常见于古代历史中，而且在工程师和物理学家为近代数学不断作出的许多贡献中更是屡见不鲜。
有记载的数学起源于东方。大约在公元前两千年，巴比伦人就搜集了极其丰富的资料，这些资料今天看来应属于初等代数的范围。至于数学作为现代意义的一门科学，则是迟至公元前5至公元前4世纪才在希腊出现的。东方和希腊之间的接触不断增多（始于波斯帝国时期，至亚历山大远征时期则达到高峰），使希腊人得以熟悉巴比伦人在数学和天文学方面的成就，数学很快就被加入到风行于希腊城邦的哲学讨论之中。因而希腊的思想家逐渐意识到，在连续、运动、无限大这些概念中，以及在用已知单位去度量任意一个量的问题中，数学都存在着固有的极大困难。面对这个挑战，经过了一番不屈不挠的努力，产生了欧多克斯（Eudoxus）的几何连续统理论，这个成果是唯一能和两千多年后的现代无理数理论相媲美的。数学中这种公理演绎的趋向起源于欧多克斯时代，又在欧几里得（Euclid）的“原本”中得以成熟。
虽然希腊数学的理论化和公理化的倾向一直是它的一个重要特点，并且曾经产生过巨大的影响。但是，对这一点我们不能过分强调，因为在古代数学中，应用以及同物理现实的联系恰恰起了同样重要的作用，而且那时候人们宁愿采用不像欧几里得那样严密的表达方式。
由于较早地发现了与“不可公度”的量有关的这些困难，使希腊人没能发展早已为东方所掌握的数字计算的技术。相反，他们却迫使自己钻进了纯粹公理几何的丛林之中。于是科学史上出现了一个奇怪的曲折。这或许意味着人类丧失了一个很好的时机。几乎两千年来，希腊几何的传统力量推迟了必然会发生的数的概念和代数运算的进步，而它们后来构成了近代科学的基础。
经过了一段缓慢的准备，到17世纪，随着解析几何与微积分的发展，数学和科学的革命也开始蓬勃发展起来。虽然希腊的几何学仍然占有重要的地位，但是，希腊人关于公理体系和系统推演的思想在17世纪和18世纪不复出现。从一些清清楚楚的定义和没有矛盾的“明显”公理出发，进行准确的逻辑推理，这对于数学科学的新的开拓者来说似乎是无关紧要的。**通过毫无拘束的直观猜想和令人信服的推理，再加上荒谬的神秘论以及对形式推理的超人力量的盲目相信，他们征服了一个蕴藏着无限财富的数学世界。**但是后来，大发展引起的狂热逐渐让位于一种自我控制的批判精神。到了19世纪，由于数学本身需要巩固已有成果，而且人们也希望把它推向更高阶段时不致发生问题（这是受到法国大革命的影响），就不得不回过头来重新审查这新的数学基础，特别是微积分及其赖以建立的极限概念。因此19世纪不仅成为一个新的发展时期，而且也以成功地返回到那种准确而严谨的证明为其特征。在这方面它甚至胜过了希腊科学的典范。于是，钟摆又一次向纯粹性和抽象性的一侧摆去。目前我们似乎仍然处于这个时期。**但是人们可以期望，在纯粹数学和具有活力的应用之间产生了这种不幸分离（可能在批判性的审查时期，这是不可避免的）之后，随之而来的应是一个紧密结合的时代。**这种重新获得的内在力量，更主要的是由于理解更加明晰而达到认识上的极大简化，将使得今天有可能在不忽略应用的情况下来掌握数学理论。再一次在纯数学和应用科学之间建立起有机的结合，在抽象的共性和色彩缤纷的个性之间建立起牢固的平衡，这或许就是不久的将来数学上的首要任务。
这里不是对数学进行详细的哲学或心理学的分析的地方。但有几点应当强调一下。目前过分强调数学的公理演绎特点的风气，似乎有盛行起来的危险。事实上，那种创造发明的要素，那种起指导和推动作用的直观要素，虽然常常不能用简单的哲学公式来表述，但是它们却是任何数学成就的核心，即使在最抽象的领域里也是如此。如果说完善的演绎形式是目标，那么直观和构作至少也是一种动力。**有一种观点对科学本身是严重的威胁，它断言数学不是别的东西，而只是从定义和公理推导出来的一组结论，而这些定义和命题除了必须不矛盾之外，可以由数学家根据他们的意志随意创造。**如果这个说法是正确的话，数学将不会吸引任何有理智的人。它将成为定义、规则和演绎法的游戏，既没有动力也没有目标。认为灵感能创造出有意义的公理体系的看法，是骗人的似是而非的真理。只有在以达到有机整体为目标的前提下，以及在内在需要的引导下，自由的思维才能作出有科学价值的成果来。
尽管逻辑分析的思辨趋势并不代表全部数学，但它却使我们对数学事实和它们相互间的依赖关系有更深刻的理解，以及对数学中的主要概念有更深刻的理解，并由此发展了可作为一般科学态度的典范的近代数学观点。
不论我们持什么样的哲学观点，就科学观察的目的来说，对一个对象的认识，完全表现在它与认识者（或仪器）的所有可能关系之中。当然仅仅是感觉并不能构成知识和见解，必须要与某些基本的实体即“自在之物”相适应、相印证，所谓“自在之物”并不是物体观察的直接对象，而是属于形而上学的。然而，对于科学方法来说，重要的是应放弃带有形而上学性质的因素，而去考虑那些可观测的事实，把它们作为概念和构作的最终根源。放弃对“自在之物”的领悟，对“终极真理”的认识以及关于世界的最终本质的阐明，这对于质朴的热诚者来说，可能会带来一种心理上的痛苦，但事实上它却是近代思想上最有成效的一种转变。
物理学上所取得的一些最伟大的成就，正是由于敢于坚持“消除形而上学”这个原则的结果。当爱因斯坦（A. Einstein）试图把“在不同地方同时发生的事件”这一概念归结为可观测的现象时，当他揭露出，认为上述概念必须有它自身的科学意义的信念只是形而上学的偏见时，他已发现了他的相对论的关键所在。当玻尔（N. Bohr）和他的学生们指出，任何物理观测必然伴随着观测工具对被观测对象的影响这个事实时，问题变得很清楚，在物理上，同时准确地确定一个粒子的位置和速度是不可能的。这个发现的深远意义体现在为每个物理学家所熟悉的近代量子力学的理论中。在十九世纪流行着一种概念，认为机械力和粒子在空间中的运动是自在之物，而电、光和磁都应当归结为力学现象或者作为力学现象来“解释”，正如以前处理“热”的方法那样。人们曾经假设过“以太”，作为一种假设性的媒介物，把它用于那些对我们来说不能完全加以解释的运动中，例如光或电。后来人们才慢慢地认识到以太是肯定无法观测到的，它属于形而上学，而不属于物理学。于是乎，在某些方面感到忧虑，而在另一些方面又感到安慰的心情下，关于光和电的力学解释连同以太最后一齐都被放弃了。
在数学中有些情况与此相类似，甚至更为突出。世世代代以来，数学家一直把他们研究的对象，例如数、点等等，看成实实在在的自在之物。但是，准确地描述这些实体的种种努力总是被这些实体自身绐否定了。从而十九世纪的数学家逐渐开始懂得，要问当作实体的这些对象究竟是什么，这是没有意义的，即使有的话也不可能在数学范围内得到解决。所有适合它们的论断都不涉及这些实体的现实，而只说明数学上“不加定义的对象”之间的相互关系以及它们所遵循的运算法则。至于点、线、数，“实际上”是什么，这不可能也不需要在数学科学中加以讨论。“可验证”的事实只是结构和关系：两点决定一直线，一些数按照某些规则组成其他一些数，等等。基本的数学概念必须抽象化，这一见解是近代公理化发展中最重要和最丰富的成果之一。
幸运的是，创造性的思维不顾某些教条的哲学信仰而继续发展着，而如果思维屈从于这种信仰就会阻碍出现建设性的成就。不论对专家来说，还是对普通人来说，唯一能回答“什么是数学”这个问题的，不是哲学而是数学本身中的活生生的经验。</description></item><item><title>Manjaro Linux访问iPhone需安装的工具</title><link>https://zenuo.github.io/fragments/2018-06-22-manjaro-linux%E8%AE%BF%E9%97%AEiphone%E9%9C%80%E5%AE%89%E8%A3%85%E7%9A%84%E5%B7%A5%E5%85%B7/</link><pubDate>Fri, 22 Jun 2018 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2018-06-22-manjaro-linux%E8%AE%BF%E9%97%AEiphone%E9%9C%80%E5%AE%89%E8%A3%85%E7%9A%84%E5%B7%A5%E5%85%B7/</guid><description>libimobiledevice简介 libimobiledevice是一个跨平台的软件库，可协议支持iPhone®，iPodTouch®，iPad®和AppleTV®设备。 与其他项目不同，它不依赖于使用任何现有的专有库并且不需要越狱。 它允许其他软件轻松访问设备的文件系统，检索有关设备及其内部的信息，备份/恢复设备，管理SpringBoard®图标，管理已安装的应用程序，检索地址簿/日历/笔记和书签以及（使用libgpod）同步音乐 和视频到设备。 该库自2007年8月开始开发，使Linux桌面支持这些设备。
以上摘自libimobiledevice项目官网
使用pacman安装 执行如下命令：
$ sudo pacman -Syy $ sudo pacman -S ifuse usbmuxd libplist libimobiledevice 参考 https://forum.manjaro.org/t/how-to-access-an-iphone-with-manjaro/3768/4 http://www.libimobiledevice.org/</description></item><item><title>使用构造方法注入依赖</title><link>https://zenuo.github.io/posts/2018/06/14/%E4%BD%BF%E7%94%A8%E6%9E%84%E9%80%A0%E6%96%B9%E6%B3%95%E6%B3%A8%E5%85%A5%E4%BE%9D%E8%B5%96/</link><pubDate>Thu, 14 Jun 2018 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/posts/2018/06/14/%E4%BD%BF%E7%94%A8%E6%9E%84%E9%80%A0%E6%96%B9%E6%B3%95%E6%B3%A8%E5%85%A5%E4%BE%9D%E8%B5%96/</guid><description>1 什么是依赖注入 在软件工程中，依赖注入是一种对象提供另一对象的依赖性的技术。依赖关系是可以使用的对象（服务）。注入是将依赖关系传递给将使用它的依赖对象（客户端）。
2 为什么使用构造方法注入 使用Spring提供的@Autowired注解，我们可以通过标记构造方法(constructor)、域(field)、setter方法和配置方法(config methods)来实现依赖注入。我们使用较多的是标记域的方式，这使得客户端可以创建该类的不合法的实例，不利于问题排查和测试，这一点在Oliver Gierke的Why field injection is evil的文章中已有说明。
3 如何使用构造方法注入 结合Lombok注解使用构造方法注入可简化代码：
@Service @Slf4j @RequiredArgsConstructor(onConstructor = @__(@Autowired)) //注意点1 public class OssSyncService { private final @NonNull //注意点2 OssSyncObjectRepository ossSyncObjectRepository; private final @NonNull OssSyncInfoRepository ossSyncInfoRepository; private final @NonNull S3Config s3Config; //其余省略 } 注意点1-类前标注@RequiredArgsConstructor(onConstructor = @__(@Autowired)) 注意点2-依赖的Bean使用private final修饰符和@NonNull注解 4 参考 http://olivergierke.</description></item><item><title>MySQL查询某张表的列名</title><link>https://zenuo.github.io/fragments/2018-05-27-mysql%E6%9F%A5%E8%AF%A2%E6%9F%90%E5%BC%A0%E8%A1%A8%E7%9A%84%E5%88%97%E5%90%8D/</link><pubDate>Sun, 27 May 2018 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2018-05-27-mysql%E6%9F%A5%E8%AF%A2%E6%9F%90%E5%BC%A0%E8%A1%A8%E7%9A%84%E5%88%97%E5%90%8D/</guid><description>查询列名，返回多列：
SELECT COLUMN_NAME FROM `information_schema`.`COLUMNS` WHERE `TABLE_SCHEMA` = &amp;#39;schema_name&amp;#39; AND TABLE_NAME = &amp;#39;table_name&amp;#39;; 查询列名，返回单列：
SELECT GROUP_CONCAT(COLUMN_NAME SEPARATOR &amp;#39;,&amp;#39;) FROM `information_schema`.`COLUMNS` WHERE `TABLE_SCHEMA` = &amp;#39;schema_name&amp;#39; AND TABLE_NAME = &amp;#39;table_name&amp;#39;; 参考 function_group-concat</description></item><item><title>MySQL的mysqldump例子</title><link>https://zenuo.github.io/fragments/2018-05-27-mysql%E7%9A%84mysqldump%E4%BE%8B%E5%AD%90/</link><pubDate>Sun, 27 May 2018 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2018-05-27-mysql%E7%9A%84mysqldump%E4%BE%8B%E5%AD%90/</guid><description>$ mysqldump \ --result-file=SQL_FILE_PATH \ --complete-insert SCHEMA_NAME [TABLE_NAME] \ -uYOUR_USERNAME \ -pYOUR_PASSWORD \ --host=HOST \ --port=PORT</description></item><item><title>快速排序C语言递归实现</title><link>https://zenuo.github.io/fragments/2017-11-09-%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8Fc%E8%AF%AD%E8%A8%80%E9%80%92%E5%BD%92%E5%AE%9E%E7%8E%B0/</link><pubDate>Thu, 09 Nov 2017 19:23:13 +0800</pubDate><guid>https://zenuo.github.io/fragments/2017-11-09-%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8Fc%E8%AF%AD%E8%A8%80%E9%80%92%E5%BD%92%E5%AE%9E%E7%8E%B0/</guid><description>#include&amp;lt;stdio.h&amp;gt; #include&amp;lt;stdlib.h&amp;gt; #define SIZE 10 void quick_sort(int a[], int lo, int hi) { if (lo &amp;lt; hi) { int i = partition(a, lo, hi); quick_sort(a, lo, i - 1); quick_sort(a, i + 1, hi); } } int partition(int a[], int lo, int hi) { int pivot, i, j, t; pivot = a[lo]; i = lo; j = hi + 1; while(1) { do ++i; while(a[i] &amp;lt;= pivot &amp;amp;&amp;amp; i &amp;lt;= hi); do --j; while(a[j] &amp;gt; pivot); if (i &amp;gt;= j) break; t = a[i], a[i] = a[j], a[j] = t; } t = a[lo], a[lo] = a[j], a[j] = t; return j; } int main(){ int array[SIZE] = {5, 4, 3, 1, -1, -3, 0, 10, 9, 8}; quick_sort(array, 0, SIZE - 1); int i; for(i = 0; i &amp;lt; SIZE; ++i) printf(&amp;#34;%d\n&amp;#34;, array[i]); return EXIT_SUCCESS; }</description></item></channel></rss>